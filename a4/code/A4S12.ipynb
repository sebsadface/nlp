{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu66L-I5tsXo"
      },
      "source": [
        "# Assignment 4: Decoding Algorithms (Section 1 and 2)\n",
        "\n",
        "- Before running the jupyter notebook, don't forget to copy it into your drive **(`File` => `Save a copy in Drive`)**. *Failing to do this step may result in losing the progress of your code.*\n",
        "- **Don't forget to choose \"Runtime Type\" = GPU in Colab for running this notebook (Runtime > Change Runtime Type > T4 GPU).**\n",
        "- For the submission of the assignment, please download this notebook as a **Python file**, named `A4S12.py`.\n",
        "\n",
        "- **You will do the following two tasks as detailed under each section, and we will grade both parts:**\n",
        "  - **Coding Exercises:** You will complete the the code blocks denoted by **`TODO:`**. We will grade your code in this notebook.\n",
        "  - **Questions to Answer:** You will answer questions denoted by **`Q:`** in your write-up.\n",
        "  \n",
        "## Section 0: Setup\n",
        "\n",
        "Please run all the code blocks in this section. You don't need to implement or change anything here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1lSg7SGaR5fM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /Users/seb/anaconda3/lib/python3.11/site-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /Users/seb/anaconda3/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/seb/anaconda3/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/seb/anaconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/seb/anaconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/seb/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/seb/anaconda3/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in /Users/seb/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/seb/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/seb/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/seb/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/seb/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/seb/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/seb/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/seb/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/seb/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/seb/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: datasets in /Users/seb/anaconda3/lib/python3.11/site-packages (2.17.1)\n",
            "Requirement already satisfied: filelock in /Users/seb/anaconda3/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/seb/anaconda3/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /Users/seb/anaconda3/lib/python3.11/site-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /Users/seb/anaconda3/lib/python3.11/site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/seb/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /Users/seb/anaconda3/lib/python3.11/site-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /Users/seb/anaconda3/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /Users/seb/anaconda3/lib/python3.11/site-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /Users/seb/anaconda3/lib/python3.11/site-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: multiprocess in /Users/seb/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/seb/anaconda3/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
            "Requirement already satisfied: aiohttp in /Users/seb/anaconda3/lib/python3.11/site-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /Users/seb/anaconda3/lib/python3.11/site-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /Users/seb/anaconda3/lib/python3.11/site-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/seb/anaconda3/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/seb/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/seb/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/seb/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/seb/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/seb/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/seb/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/seb/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/seb/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/seb/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/seb/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/seb/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/seb/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/seb/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /Users/seb/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: evaluate in /Users/seb/anaconda3/lib/python3.11/site-packages (0.4.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /Users/seb/anaconda3/lib/python3.11/site-packages (from evaluate) (2.17.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/seb/anaconda3/lib/python3.11/site-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /Users/seb/anaconda3/lib/python3.11/site-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: pandas in /Users/seb/anaconda3/lib/python3.11/site-packages (from evaluate) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /Users/seb/anaconda3/lib/python3.11/site-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /Users/seb/anaconda3/lib/python3.11/site-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /Users/seb/anaconda3/lib/python3.11/site-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: multiprocess in /Users/seb/anaconda3/lib/python3.11/site-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /Users/seb/anaconda3/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /Users/seb/anaconda3/lib/python3.11/site-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /Users/seb/anaconda3/lib/python3.11/site-packages (from evaluate) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /Users/seb/anaconda3/lib/python3.11/site-packages (from evaluate) (0.13.3)\n",
            "Requirement already satisfied: filelock in /Users/seb/anaconda3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /Users/seb/anaconda3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /Users/seb/anaconda3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /Users/seb/anaconda3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/seb/anaconda3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/seb/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/seb/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/seb/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/seb/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/seb/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: six in /Users/seb/anaconda3/lib/python3.11/site-packages (from responses<0.19->evaluate) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/seb/anaconda3/lib/python3.11/site-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/seb/anaconda3/lib/python3.11/site-packages (from pandas->evaluate) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/seb/anaconda3/lib/python3.11/site-packages (from pandas->evaluate) (2023.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/seb/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/seb/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/seb/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/seb/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/seb/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "H5S6UO84y3fj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: mps\n"
          ]
        }
      ],
      "source": [
        "\"\"\"set device and random seeds\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper functions are given to you.\n",
        "######################################################\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "print(f'device: {device}')\n",
        "\n",
        "def set_seed(seed=19260817):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S8uX6e1uCEU"
      },
      "source": [
        "### 0.1 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kPcsD3_Gqm7P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'story_id': '080198fc-d0e7-42b3-8e63-b2144e59d816', 'prompt': 'On my way to work I stopped to get some coffee.', 'continuation': 'I went through the drive through and placed my order. I paid the cashier and patiently waited for my drink. When she handed me the drink, the lid came off and spilled on me. The coffee hurt and I had to go home and change clothes.', 'constraint_words': ['drive', 'order', 'drink', 'lid', 'coffee', 'hurt', 'home', 'change', 'clothes']}\n"
          ]
        }
      ],
      "source": [
        "\"\"\"load datasets\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('Ximing/ROCStories')\n",
        "train_data, dev_data, test_data = dataset['train'], dataset['validation'], dataset['test']\n",
        "\n",
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_dqyjUBuQlQ"
      },
      "source": [
        "### 0.2 Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gmMd57VjSEZY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at textattack/roberta-base-CoLA were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "\"\"\"prepare evaluation\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from evaluate import load\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "\n",
        "perplexity_scorer = load(\"perplexity\", module_type=\"metric\")\n",
        "cola_model_name = \"textattack/roberta-base-CoLA\"\n",
        "cola_tokenizer = RobertaTokenizer.from_pretrained(cola_model_name)\n",
        "cola_model = RobertaForSequenceClassification.from_pretrained(cola_model_name).to(device)\n",
        "\n",
        "def batchify(data, batch_size):\n",
        "    assert batch_size > 0\n",
        "\n",
        "    batch = []\n",
        "    for item in data:\n",
        "        # Yield next batch\n",
        "        if len(batch) == batch_size:\n",
        "            yield batch\n",
        "            batch = []\n",
        "\n",
        "        batch.append(item)\n",
        "\n",
        "    # Yield last un-filled batch\n",
        "    if len(batch) != 0:\n",
        "        yield batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7t97XqNh2Xu2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "641ca6ca113344f6a7778852050c4a18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "debugging run\n",
            "perplexity = 178.62\n",
            "fluency = 0.98\n",
            "diversity = 0.87, 1.00, 1.00\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"set up evaluation metric\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "def compute_perplexity(texts, model='gpt2', batch_size=8):\n",
        "    score = perplexity_scorer.compute(predictions=texts, add_start_token=True, batch_size=batch_size, model_id=model)\n",
        "    return score['mean_perplexity']\n",
        "\n",
        "\n",
        "def compute_fluency(texts, batch_size=8):\n",
        "  scores = []\n",
        "  for b_texts in batchify(texts, batch_size):\n",
        "    inputs = cola_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "      logits = cola_model(**inputs).logits\n",
        "      probs = logits.softmax(dim=-1)\n",
        "      scores.extend(probs[:, 1].tolist())\n",
        "  return sum(scores) / len(scores)\n",
        "\n",
        "\n",
        "def compute_diversity(texts):\n",
        "    unigrams, bigrams, trigrams = [], [], []\n",
        "    total_words = 0\n",
        "    for gen in texts:\n",
        "        o = gen.split(' ')\n",
        "        total_words += len(o)\n",
        "        for i in range(len(o)):\n",
        "            unigrams.append(o[i])\n",
        "        for i in range(len(o) - 1):\n",
        "            bigrams.append(o[i] + '_' + o[i + 1])\n",
        "        for i in range(len(o) - 2):\n",
        "            trigrams.append(o[i] + '_' + o[i + 1] + '_' + o[i + 2])\n",
        "    return len(set(unigrams)) / len(unigrams), len(set(bigrams)) / len(bigrams), len(set(trigrams)) / len(trigrams)\n",
        "\n",
        "\n",
        "def evaluate(generations, experiment):\n",
        "  generations = [_ for _ in generations if _ != '']\n",
        "  perplexity = compute_perplexity(generations)\n",
        "  fluency = compute_fluency(generations)\n",
        "  diversity = compute_diversity(generations)\n",
        "  print(experiment)\n",
        "  print(f'perplexity = {perplexity:.2f}')\n",
        "  print(f'fluency = {fluency:.2f}')\n",
        "  print(f'diversity = {diversity[0]:.2f}, {diversity[1]:.2f}, {diversity[2]:.2f}')\n",
        "  print()\n",
        "\n",
        "debug_sents = [\"This restaurant is awesome\", \"My dog is cute and I love it.\", \"Today is sunny.\"]\n",
        "evaluate(debug_sents, 'debugging run')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGaWrCTzzIqA"
      },
      "source": [
        "### 0.3: Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cn5_8Bym3HAq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"load model and tokenizer\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name, pad_token=\"<|endoftext|>\")\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfwAdXU4urVp"
      },
      "source": [
        "## **Section 1: Basic Decoding Algorithms**\n",
        "\n",
        "In this section, you will implement a few basic decoding algorithms:\n",
        "1. Greedy decoding\n",
        "2. Vanilla sampling\n",
        "3. Temperature sampling\n",
        "4. Top-k sampling\n",
        "5. Top-p sampling\n",
        "\n",
        "We have provided a wrapper function `decode()` that takes care of batching, controlling max length, and handling the EOS token.\n",
        "You will be asked to implement the core function of each method: *given the pre-softmax logits of the next token, decide what the next token is.*\n",
        "\n",
        "**The wrapper calls the core function of each decoding algorithm, which you will implement in the subsections below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "L2SG4_z91dJu"
      },
      "outputs": [],
      "source": [
        "\"\"\"decode main wrapper function\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "def decode(prompts, max_len, method, **kwargs):\n",
        "  encodings_dict = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
        "  input_ids = encodings_dict['input_ids'].to(device)\n",
        "  attention_mask = encodings_dict['attention_mask'].to(device)\n",
        "\n",
        "  model_kwargs = {'attention_mask': attention_mask}\n",
        "  batch_size, input_seq_len = input_ids.shape\n",
        "\n",
        "  unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=device)\n",
        "\n",
        "  for step in range(max_len):\n",
        "    model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**model_inputs, return_dict=True, output_attentions=False, output_hidden_states=False)\n",
        "\n",
        "    if step == 0:\n",
        "      last_non_masked_idx = torch.sum(attention_mask, dim=1) - 1\n",
        "      next_token_logits = outputs.logits[range(batch_size), last_non_masked_idx, :]\n",
        "    else:\n",
        "      next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "    log_prob = F.log_softmax(next_token_logits, dim=-1)\n",
        "\n",
        "    if method == 'greedy':\n",
        "      next_tokens = greedy(next_token_logits)\n",
        "    elif method == 'sample':\n",
        "      next_tokens = sample(next_token_logits)\n",
        "    elif method == 'temperature':\n",
        "      next_tokens = temperature(next_token_logits, t=kwargs.get('t', 0.8))\n",
        "    elif method == 'topk':\n",
        "      next_tokens = topk(next_token_logits, k=kwargs.get('k', 20))\n",
        "    elif method == 'topp':\n",
        "      next_tokens = topp(next_token_logits, p=kwargs.get('p', 0.7))\n",
        "\n",
        "    # finished sentences should have their next token be a padding token\n",
        "    next_tokens = next_tokens * unfinished_sequences + tokenizer.pad_token_id * (1 - unfinished_sequences)\n",
        "\n",
        "    input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
        "    model_kwargs = model._update_model_kwargs_for_generation(outputs, model_kwargs, is_encoder_decoder=model.config.is_encoder_decoder)\n",
        "\n",
        "    # if eos_token was found in one sentence, set sentence to finished\n",
        "    unfinished_sequences = unfinished_sequences.mul((next_tokens != tokenizer.eos_token_id).long())\n",
        "\n",
        "    if unfinished_sequences.max() == 0:\n",
        "      break\n",
        "\n",
        "  response_ids = input_ids[:, input_seq_len:]\n",
        "  response_text = [tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True) for output in response_ids]\n",
        "\n",
        "  return response_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "P-bghfZe30wN"
      },
      "outputs": [],
      "source": [
        "\"\"\"debug helper code\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "# For debugging, we duplicate a single prompt 10 times so that we obtain 10 generations for the same prompt\n",
        "dev_prompts = [dev_data[0]['prompt']] * 10\n",
        "\n",
        "def print_generations(prompts, generations):\n",
        "  for prompt, generation in zip(prompts, generations):\n",
        "    print(f'{[prompt]} ==> {[generation]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ryFGrlRSXYn"
      },
      "source": [
        "### 1.1: Greedy Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xH0wBhy2SZa-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n"
          ]
        }
      ],
      "source": [
        "def greedy(next_token_logits):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # TODO: compute `next_tokens` from `next_token_logits`.\n",
        "  # Hint: use torch.argmax()\n",
        "  next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "generations = decode(dev_prompts, max_len=20, method='greedy')\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGKL_31VJNw1"
      },
      "source": [
        "### 1.2: Vanilla Sampling and Temperature Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PjrTLKj2JR5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ryan was called by his friend to skip work one day.'] ==> [\" (Store, Voc and J.STYLE/Scott's Employees Connected your Daughter) Within\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nIn numerous comments, the N.Y.U. assistant general counsel pointed out that there']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nA blunt reception poured down from Philadelphia Police brandishing handguns and 500 Police Sgt. LaR']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nThen came a nagging run of infections that starting on Friday, March 9. Shortly after']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' But as he stormed out of Cornell for Ohio on Friday, he decided to take the wrong bus.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nDr McGrath arrested him the next day at 14900 High Street. Thereafter, it']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' «Don\\'t go! Don\\'t go! Watch that sailor down my pants!!\">«She was']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' Gerriers local girl was yelling \"cowardly,\" which made Joey appear under the guise of telling']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' \"Expecting a much better job than that, to be out of school the whole day, to']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nADVERTISEMENT\\n\\nSmyth told him his boss was struggling to pay worth of tuition.']\n"
          ]
        }
      ],
      "source": [
        "def sample(next_token_logits):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # TODO: compute the probabilities `probs` from the logits.\n",
        "  # Hint: `probs` should have size (B, V)\n",
        "  probs = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "  # TODO: compute `next_tokens` from `probs`.\n",
        "  # Hint: use torch.multinomial()\n",
        "  next_tokens = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='sample')\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "25Su03uzJb_Z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ryan was called by his friend to skip work one day.'] ==> [' (Store, 37-4)\\n\\nAfter falling asleep and missing school on the way home,']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nIn an interview with the Los Angeles Times, Daschle said he regretted dropping out of']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nA week later, the trainer was at the same gym and had been hit with a nasty']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"He\\'ll be back late next year,\" Hamilton said.\\n\\n- With files from']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' But as he stormed out of the studio, on his way to leave for a drive, he ran']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nA similar court case was dismissed on the same grounds.\\n\\nDell also wanted to']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' He did not. And as the feds do not like the fact that he is a Muslim, he']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' He was out of town then. They played a game of \"I-don\\'t-want-']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' \"I went home and cried, and then I was doing little things,\" the 26-year-']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nThe boy shared with friends that it was his first time struggling to make ends meet.\\n']\n"
          ]
        }
      ],
      "source": [
        "def temperature(next_token_logits, t):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  - t: float\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # TODO: compute the probabilities `probs` from the logits, with temperature applied.\n",
        "  probs = F.softmax(next_token_logits / t, dim=-1)\n",
        "\n",
        "  # TODO: compute `next_tokens` from `probs`.\n",
        "  next_tokens = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='temperature', t=0.8)\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZZorDeWRVtm"
      },
      "source": [
        "### 1.3: Top-k Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hNHk9mjXRdNA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ryan was called by his friend to skip work one day.'] ==> [' (I think he did it for the last minute to avoid the consequences of what he did, since']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nIn an interview with the Los Angeles Times, Lefebvre told CNN that he would']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nA week later, they met at her house, and then went for a walk to take']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"So I walked home late after work and I was sitting in my car and I went']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' But as he headed to work, it became clear he hadn\\'t been seen. \"I\\'m not']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nIt was a Friday night, and the couple was headed to their hotel in the Hampt']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" He did not. And as it turns out, that's because he had no clue his son was\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' \"I\\'m just not sure if I feel any obligation to go back to work,\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' \"I went home and cried on the couch because my dad and I were so close, and he']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nThe boy was working with an older man in the restaurant and told him, \"I\\'m']\n"
          ]
        }
      ],
      "source": [
        "def topk(next_token_logits, k):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  - k: int\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # TODO: Keep only top-k tokens with highest probabilities.\n",
        "  # Hint: use torch.topk()\n",
        "  topk_logits, topk_indices = torch.topk(next_token_logits, k, dim=-1)\n",
        "\n",
        "  # Create a mask to zero out all logits not in top-k\n",
        "  indices_to_remove = next_token_logits < topk_logits[:, -1].unsqueeze(1)\n",
        "\n",
        "  # Mask the logits\n",
        "  next_token_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "  # TODO: Sample from the masked logits\n",
        "  probs = F.softmax(next_token_logits, dim=-1)\n",
        "  next_tokens = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='topk', k=20)\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSjMWNFEy_cC"
      },
      "source": [
        "### 1.4: Top-p Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Oq1ZwVVxzApa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ryan was called by his friend to skip work one day.'] ==> [' (I think he did it for the last minute to avoid attention.) \"I\\'m sorry, dude']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\"\\n\\nIn an interview with the Hartford Courant on Tuesday, Terry Frith said he wasn't\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\"\\n\\nA week later, Gray said, her sister was working on the school's financial aid,\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nThen came a nagging letter from his boss and he was the subject of the police report']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" But as he headed to work, it became clear he didn't want to go home.\\n\\n\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nIt was a short while after Christmas, when he began to panic. He lost consciousness and']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" He did not. And as it turns out, that's because he had no clue that he was\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' \"I\\'m just too tired,\" he said. \"If I go to bed, I\\'ll get']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' \"I went home and cried, like a baby,\" said Sarah. \"And I can\\'t even']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\nThe boy was brought home by a man who also was a father.\\n\\nThe incident']\n"
          ]
        }
      ],
      "source": [
        "def topp(next_token_logits, p):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  - p: float\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # TODO: Sort the logits in descending order, and compute\n",
        "  # the cumulative probabilities `cum_probs` on the sorted logits\n",
        "  sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "  sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
        "  cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "  # Create a mask to zero out all logits not in top-p\n",
        "  sorted_indices_to_remove = cum_probs > p\n",
        "  sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "  sorted_indices_to_remove[:, 0] = 0\n",
        "  # Restore mask to original indices\n",
        "  indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "\n",
        "  # Mask the logits\n",
        "  next_token_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "  # TODO: Sample from the masked logits\n",
        "  probs = F.softmax(next_token_logits, dim=-1)\n",
        "  next_tokens = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='topp', p=0.7)\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZElDTWEHCHNO"
      },
      "source": [
        "### 1.5: Evaluate!\n",
        "\n",
        "Run the following cell to obtain the evaluation results, which you should include in your writeup.\n",
        "Also don't forget to answer the questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2UjZ-31s449u"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c35fe4b1e474821832606f9dea3a9db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m tqdm(prompts):\n\u001b[0;32m----> 8\u001b[0m   generations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m decode([prompt] \u001b[38;5;241m*\u001b[39m GENERATIONS_PER_PROMPT, max_len\u001b[38;5;241m=\u001b[39mMAX_LEN, method\u001b[38;5;241m=\u001b[39mexperiment)\n\u001b[1;32m      9\u001b[0m evaluate(generations, experiment)\n",
            "Cell \u001b[0;32mIn[7], line 20\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(prompts, max_len, method, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 20\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     23\u001b[0m   last_non_masked_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(attention_mask, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[1;32m   1075\u001b[0m     input_ids,\n\u001b[1;32m   1076\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1077\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1078\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1079\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1080\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1081\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1082\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1083\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m   1084\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1085\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1086\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1087\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1088\u001b[0m )\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m block(\n\u001b[1;32m    889\u001b[0m         hidden_states,\n\u001b[1;32m    890\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[1;32m    891\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    892\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask[i],\n\u001b[1;32m    893\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m    894\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m    895\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    896\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    897\u001b[0m     )\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:389\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    379\u001b[0m     hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    386\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    387\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]]:\n\u001b[1;32m    388\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 389\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[1;32m    390\u001b[0m     attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[1;32m    391\u001b[0m         hidden_states,\n\u001b[1;32m    392\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    397\u001b[0m     )\n\u001b[1;32m    398\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlayer_norm(\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalized_shape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:2546\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2544\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2545\u001b[0m     )\n\u001b[0;32m-> 2546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlayer_norm(\u001b[38;5;28minput\u001b[39m, normalized_shape, weight, bias, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "prompts = [item['prompt'] for item in test_data][:10]\n",
        "GENERATIONS_PER_PROMPT = 10\n",
        "MAX_LEN = 100\n",
        "\n",
        "for experiment in ['greedy', 'sample', 'temperature', 'topk', 'topp']:\n",
        "  generations = []\n",
        "  for prompt in tqdm(prompts):\n",
        "    generations += decode([prompt] * GENERATIONS_PER_PROMPT, max_len=MAX_LEN, method=experiment)\n",
        "  evaluate(generations, experiment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv0P_mt1RWh6"
      },
      "source": [
        "## **Section 2: Beam Search**\n",
        "\n",
        "- In this part of the assignment, your main task is to implement beam search algorithm. While debugging for implementation, we recommend setting `device` to `cpu`, as debugging would not require too much resource for this assignment. Once you are done with implementation, you may switch `device` to `cuda` and choose `Colab Runtime Type = GPU` for faster evaluation of your algorithm.\n",
        "\n",
        "- In the following part, we have provided some helpful functions and classes. Please make sure that you understand them. But you don't need to implement/change anything of them until **Section 2.1**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPbvq4LnRd_j"
      },
      "source": [
        "### Configurations: load model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "78dmI0HW4P-h"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GenerationConfig\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name, pad_token=\"<|endoftext|>\")\n",
        "tokenizer.padding_side = \"left\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "model.eval()\n",
        "\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "eos_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq9lOjrwRSrs"
      },
      "source": [
        "### Helper classes: `BeamHypothesisList` and `BeamManager`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ut9kXr0LRf4n"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "#  Try your best to understand the code,\n",
        "#  but you don't need to implement anything here.\n",
        "######################################################\n",
        "\n",
        "@dataclass\n",
        "class BeamHypothesis:\n",
        "    def __init__(self, input_ids: torch.LongTensor, score: torch.FloatTensor | float):\n",
        "        self.input_ids: torch.LongTensor = input_ids  # a single token sequence of size (seq_len,)\n",
        "        self.score: torch.FloatTensor = score  # a scalar score for the token sequence\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"BeamHypothesis(input_ids: {self.input_ids}, score: {self.score})\"\n",
        "\n",
        "class BeamHypothesisList:\n",
        "    def __init__(self, num_beams: int):\n",
        "        self.beam_hypotheses: List[BeamHypothesis] = []  # list of beam_hypothesis\n",
        "        self.num_beams: int = num_beams\n",
        "\n",
        "        self.worst_score = 1e9  # worst beam score in self.beam_hypotheses\n",
        "\n",
        "    def add(self, new_input_ids: torch.LongTensor, sum_logprobs: float):\n",
        "        \"\"\"\n",
        "        :param new_input_ids: new token sequence of size (1, seq_len)\n",
        "        :param sum_logprobs: sum of log probabilities of tokens in new_input_ids\n",
        "        Given a new hypothesis (new_input_ids) and its corresponding sum_logprobs, update self.beam_hypotheses with a finished hypothesis.\n",
        "        (1) If the new_input_ids has higher score than the current worst score in self.beam_hypotheses,\n",
        "            we replace the worst one with the new hypothesis.\n",
        "        (2) Otherwise, we do not make any change to self.beam_hypotheses.\n",
        "        \"\"\"\n",
        "        # For score, we compute average token log probability\n",
        "        score = sum_logprobs / new_input_ids.size(-1)\n",
        "\n",
        "        # add the new hypothesis if we still have vacant beams\n",
        "        if len(self.beam_hypotheses) < self.num_beams:\n",
        "            # Initialize the new_beam_hypothesis using new_input_ids and score\n",
        "            new_beam_hypothesis: BeamHypothesis = BeamHypothesis(input_ids=new_input_ids, score=score)\n",
        "\n",
        "            # Add new_beam_hypothesis to beam_hypotheses\n",
        "            self.beam_hypotheses.append(new_beam_hypothesis)\n",
        "\n",
        "        # even if the beam_hypotheses are full, if the new hypothesis has higher score than the worst hypothesis, we replace the worst hypothesis\n",
        "        elif score > self.worst_score:\n",
        "            # Remove the worst hypothesis, the one with the lowest score in self.beam_hypotheses\n",
        "            worst_hypothesis: BeamHypothesis = min(self.beam_hypotheses, key=lambda hyp: hyp.score)\n",
        "            self.beam_hypotheses.remove(worst_hypothesis)\n",
        "\n",
        "            # Add new hypothesis\n",
        "            # Initialize the new_beam_hypothesis using new_input_ids and score\n",
        "            new_beam_hypothesis = BeamHypothesis(input_ids=new_input_ids, score=score)\n",
        "\n",
        "            # Add new_beam_hypothesis to beam_hypotheses\n",
        "            self.beam_hypotheses.append(new_beam_hypothesis)\n",
        "\n",
        "        # Update the worst score - the lowest score among all beams in self.beam_hypotheses\n",
        "        self.worst_score = min(float(hyp.score) for hyp in self.beam_hypotheses)\n",
        "\n",
        "        # Sanity check\n",
        "        assert len(self.beam_hypotheses) <= self.num_beams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2HD5CoYBSvpE"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "#  Try your best to understand the code,\n",
        "#  but you don't need to implement anything here.\n",
        "######################################################\n",
        "\n",
        "class BeamManager:\n",
        "    def __init__(self, batch_size: int, num_beams: int):\n",
        "        self.finished_beam_hypotheses_list = [BeamHypothesisList(num_beams) for _ in range(batch_size)]\n",
        "        self.batch_size = batch_size\n",
        "        self.num_beams = num_beams\n",
        "\n",
        "    def process(self,\n",
        "                input_ids: torch.LongTensor,\n",
        "                top_token_scores: torch.FloatTensor,\n",
        "                top_token_indices: torch.LongTensor,\n",
        "                top_token_beam_indices: torch.LongTensor\n",
        "                ):\n",
        "        \"\"\"\n",
        "        :param input_ids: (batch_size * num_beams, current_seq_length), the input_ids that were used to compute top_tokens\n",
        "        :param top_token_scores: (batch_size, 2 * num_beams), representing the score of each top token\n",
        "        :param top_token_indices: (batch_size, 2 * num_beams), representing each token's index (in vocabulary) of the top tokens\n",
        "        :param top_token_beam_indices: (batch_size, 2 * num_beams), representing each token's corresponding beam index of the top tokens\n",
        "\n",
        "        Note: the input arguments `top_token_*` for each sample in batch are sorted from the largest score to the smallest score.\n",
        "        For example, if batch_size = 2 and num_beams = 3, then each of these values denote\n",
        "        top_token_indices[1, 2]: what is the third-best next token for the second sample in the batch?\n",
        "        top_token_scores[0, 1]: what is the score of the second-best next token for the first sample in the batch?\n",
        "        top_token_beam_indices[0, 1]: which beam did we use to generate the second-best next token for the first sample in the batch?\n",
        "\n",
        "        In this function, for each of the top-(2 * num_beams) tokens, we do the following:\n",
        "        (1) If the top token is EOS token:\n",
        "            This means that this hypothesis is done. Therefore, we save the hypothesis so-far to self.finished_beam_hypotheses_list.\n",
        "        (2) If the top token is not EOS token\n",
        "            We have to keep searching with this hypothesis. Therefore, we prepare the hypothesis for next time step.\n",
        "\n",
        "        Returns a dictionary, where\n",
        "        \"unfinished_scores\": size (batch_size * num_beams,), the score of the unfinished beams\n",
        "        \"unfinished_token_indices\": size (batch_size * num_beams,), the index of the last token in the unfinished beams\n",
        "        \"unfinished_beam_indices\": the index of the beam that was used to generate the new unfinished beam\n",
        "        \"\"\"\n",
        "        device = top_token_scores.device\n",
        "\n",
        "        # Initialize unfinished_token_*, which we will return for the next time step.\n",
        "        unfinished_scores = torch.zeros((self.batch_size, self.num_beams), dtype=top_token_scores.dtype).to(device)  # score of the unfinished beams\n",
        "        unfinished_token_indices = torch.zeros((self.batch_size, self.num_beams), dtype=top_token_indices.dtype).to(\n",
        "            device)  # index of the last token of the unfinished beams\n",
        "        unfinished_token_beam_indices = torch.zeros((self.batch_size, self.num_beams), dtype=top_token_beam_indices.dtype).to(\n",
        "            device)  # index of the unfinished beam in the batch\n",
        "\n",
        "        # Loop over the batch\n",
        "        for batch_idx in range(self.batch_size):\n",
        "            # Get sample_beam_hypothesis_list: the finished_beam_hypothesis_list for this sample in the batch\n",
        "            sample_beam_hypothesis_list: BeamHypothesisList = self.finished_beam_hypotheses_list[batch_idx]\n",
        "\n",
        "            # Get the top_token_scores, top_token_indices, top_token_beam_indices for this sample in the batch\n",
        "            # NOTE: size of sample_top_token_*: (2 * num_beams,)\n",
        "            sample_top_token_scores = top_token_scores[batch_idx]\n",
        "            sample_top_token_indices = top_token_indices[batch_idx]\n",
        "            sample_top_token_beam_indices = top_token_beam_indices[batch_idx]\n",
        "\n",
        "            # Loop over all top tokens\n",
        "            sample_beam_idx = 0\n",
        "            for top_token_score, top_token_index, top_token_beam_index in zip(\n",
        "                    sample_top_token_scores, sample_top_token_indices, sample_top_token_beam_indices\n",
        "            ):\n",
        "                # Note that top_token_beam_indices only denotes the index of the beam in each sample.\n",
        "                # We transform this into `beam_idx_in_batch`, where we denote the index of the beam among all (batch_size * num_beams) beams in the batch.\n",
        "                beam_idx_in_batch = batch_idx * self.num_beams + top_token_beam_index\n",
        "\n",
        "                # if top_token == EOS, we add the generation so-far to the beam_hypotheses_list\n",
        "                if top_token_index.item() == eos_token_id:\n",
        "                    # Among the (batch_size * num_beams) input_ids, find the input_ids that correspond to this top_token\n",
        "                    # NOTE: the size of new_input_ids: (seq_len,)\n",
        "                    new_input_ids = input_ids[beam_idx_in_batch]\n",
        "\n",
        "                    # Add the new beam to sample_beam_hypothesis_list\n",
        "                    sample_beam_hypothesis_list.add(\n",
        "                        new_input_ids,\n",
        "                        top_token_score\n",
        "                    )\n",
        "\n",
        "                # if top_token =/= EOS, we aggregate them for next time step.\n",
        "                else:\n",
        "                    # Store the score, token_index, beam_idx_in_batch to the unfinished_scores, unfinished_token_indices, unfinished_token_beam_indices\n",
        "                    unfinished_scores[batch_idx, sample_beam_idx] = top_token_score\n",
        "                    unfinished_token_indices[batch_idx, sample_beam_idx] = top_token_index\n",
        "                    unfinished_token_beam_indices[batch_idx, sample_beam_idx] = beam_idx_in_batch\n",
        "\n",
        "                    sample_beam_idx += 1\n",
        "\n",
        "                # once we have `num_beams` number of new beams, we don't have to add anymore.\n",
        "                if sample_beam_idx == self.num_beams:\n",
        "                    break\n",
        "\n",
        "        # Return the dictionary of unfinished_scores, unfinished_token_indices, unfinished_beam_indices\n",
        "        # Make sure to change the size of each tensor to (batch_size * num_beams,)\n",
        "        return {\n",
        "            \"unfinished_scores\": unfinished_scores.view(-1),  # (batch_size * num_beams,)\n",
        "            \"unfinished_token_indices\": unfinished_token_indices.view(-1),  # (batch_size * num_beams,)\n",
        "            \"unfinished_beam_indices\": unfinished_token_beam_indices.view(-1),  # (batch_size * num_beams,)\n",
        "        }\n",
        "\n",
        "    def finalize(\n",
        "            self,\n",
        "            input_ids: torch.LongTensor,\n",
        "            beam_scores: torch.FloatTensor,\n",
        "    ) -> Tuple[List[torch.LongTensor], List[torch.FloatTensor]]:\n",
        "        \"\"\"\n",
        "        :param input_ids: (batch_idx * num_beams, max_length), input_ids of unfinished beams\n",
        "        :param beam_scores: (batch_idx * num_beams,), scores of unfinished beams\n",
        "        Get the final best beams, among\n",
        "        (1) unfinished beams, for which we get the input_ids and beam_scores as arguments\n",
        "        (2) finished beams, which we store in self.batch_beam_hypothesis_list\n",
        "        Returns a tuple of two lists, where\n",
        "        - tuple[0] is the list of the input_ids of the best beams (length: batch_idx)\n",
        "        - tuple[1] is the list of the scores of the best beams (length: batch_idx\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. Add all unfinished beam hypotheses to self.finished_beam_hypotheses_list\n",
        "        for batch_idx in range(self.batch_size):\n",
        "            # Get sample_beam_hypothesis_list: the finished_beam_hypothesis_list for this sample in the batch\n",
        "            sample_beam_hypothesis_list: BeamHypothesisList = self.finished_beam_hypotheses_list[batch_idx]\n",
        "\n",
        "            for sample_beam_idx in range(self.num_beams):\n",
        "                # Get beam_idx_in_batch: index of the beam in all `batch_size * num_beams` beams in the batch\n",
        "                beam_idx_in_batch = batch_idx * self.num_beams + sample_beam_idx\n",
        "\n",
        "                # Get the input_id for this beam, using `beam_idx_in_batch`\n",
        "                # NOTE: the size of new_input_ids: (seq_len,)\n",
        "                new_input_ids = input_ids[beam_idx_in_batch]\n",
        "\n",
        "                # Get the score of this beam, using `beam_idx_in_batch`\n",
        "                # NOTE: beam_score should be a scalar\n",
        "                beam_score = beam_scores[beam_idx_in_batch].item()\n",
        "\n",
        "                # Add the new hypothesis to sample_beam_hypothesis_list\n",
        "                sample_beam_hypothesis_list.add(new_input_ids, beam_score)\n",
        "\n",
        "        # 2. Select the best hypothesis from each beam_hypothesis_list\n",
        "        best_input_ids = []\n",
        "        best_scores = []\n",
        "        for batch_idx in range(self.batch_size):\n",
        "            # Get sample_beam_hypothesis_list: the finished_beam_hypothesis_list for this sample in the batch\n",
        "            sample_beam_hypothesis_list: BeamHypothesisList = self.finished_beam_hypotheses_list[batch_idx]\n",
        "\n",
        "            # Get best_hypothesis among sample_beam_hypothesis_list (the one with the highest score)\n",
        "            best_hypothesis = max(sample_beam_hypothesis_list.beam_hypotheses, key=lambda hyp: hyp.score)\n",
        "\n",
        "            # Save the input_ids and score of best_hypothesis\n",
        "            best_input_ids.append(best_hypothesis.input_ids)\n",
        "            best_scores.append(best_hypothesis.score)\n",
        "\n",
        "        return best_input_ids, best_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXle5lWRRVma"
      },
      "source": [
        "### 2.1 Beam Search\n",
        "\n",
        "**TODO: fill in the todos in `beam_search`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "9JTN6KpYS0P-"
      },
      "outputs": [],
      "source": [
        "def beam_search(prompts: List[str], num_beams: int, max_length: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    :param prompts: list of prompt strings\n",
        "    :param num_beams: number of beams\n",
        "    :param max_length: max generation length\n",
        "    :return: list of generation, including both the original prompt and generation\n",
        "    \"\"\"\n",
        "    # TODO: encode the prompts using tokenizer, to get input_ids and attention_mask\n",
        "    input_encoding = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
        "    input_ids, attention_mask = input_encoding[\"input_ids\"].to(device), input_encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "    if input_ids.size(-1) > max_length:\n",
        "      raise ValueError(\"Input ID is larger than max_length.\")\n",
        "\n",
        "    # --- Do not change below --- #\n",
        "    batch_size = input_ids.size(0)\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # initialize model_kwargs\n",
        "    model_kwargs = {'attention_mask': attention_mask}\n",
        "\n",
        "    # interleave input_ids according to num_beams.\n",
        "    # For example, input_ids for [\"Hi\", \"good\"] with num_beams=3 becomes [\"Hi\", \"Hi\", \"Hi\", \"good\", \"good\", \"good\"]\n",
        "    input_ids, model_kwargs = model._expand_inputs_for_generation(\n",
        "        input_ids=input_ids,\n",
        "        expand_size=num_beams,\n",
        "        is_encoder_decoder=False,\n",
        "        **model_kwargs,\n",
        "    )\n",
        "    # NOTE: the type of `input_ids` and `model_kwargs` are as following:\n",
        "    # input_ids: tensor of size (batch_size * num_beams, seq_len)\n",
        "    # model_kwargs: a dictionary with single element 'attention_mask', sized (batch_size * num_beams, seq_len)\n",
        "    # --- Do not change above --- #\n",
        "\n",
        "    # TODO: initialize beam_manager\n",
        "    beam_manager = BeamManager(batch_size, num_beams)\n",
        "\n",
        "    # TODO: initialize unfinished_beam_scores, a tensor of size (batch_size, num_beams) with all elements = 0\n",
        "    unfinished_beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float).to(device)\n",
        "\n",
        "    # For each sample in the batch, set all initial beam_score to -1e9, except for the first beam\n",
        "    unfinished_beam_scores[:, 1:] = -1e9\n",
        "    unfinished_beam_scores = unfinished_beam_scores.view(-1)  # (batch_size * num_beams,)\n",
        "\n",
        "    while True:\n",
        "        # --- Do not change below --- #\n",
        "        model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "        # --- Do not change above --- #\n",
        "\n",
        "        # TODO: run model forward pass with model_inputs as the input\n",
        "        # NOTE: we should set return_dict=True, output_attentions=False and output_hidden_states=False\n",
        "        model_outputs = model(**model_inputs, return_dict=True, output_attentions=False, output_hidden_states=False)\n",
        "\n",
        "        # TODO compute log_probs for next tokens, using `model_outputs.logits`\n",
        "        # NOTE: size of next_token_scores: (batch_size * num_beams, vocab_size)\n",
        "        next_token_logits = model_outputs.logits[:, -1, :]\n",
        "        next_token_scores = F.log_softmax(next_token_logits, dim=-1)\n",
        "\n",
        "        # TODO add previous beam_scores to the next_token_scores\n",
        "        # NOTE: size of new_scores: # (batch_size * num_beams, vocab_size)\n",
        "        new_scores = next_token_scores + unfinished_beam_scores.unsqueeze(-1)\n",
        "\n",
        "        # TODO: retrieve top-(2 * num_beams) next tokens for each sample in the batch\n",
        "        # NOTE: size of `top_token_scores` and `top_token_indices` needs to be: (batch_size, 2 * num_beams)\n",
        "        # NOTE: `top_token_scores` and `top_token_indices` should be sorted from the one with larget score to the one with smallest score (for each sample in batch)\n",
        "        # Hint: use torch.topk with largest=True, sorted=True\n",
        "        # Hint: new_scores needs to be transformed to shape (batch_size, num_beams * vocab_size) prior to topk operation.\n",
        "        top_token_scores, top_token_indices = torch.topk(new_scores.view(batch_size, -1), 2 * num_beams, dim=-1, largest=True, sorted=True)\n",
        "\n",
        "        # Since top_token_indices are over num_beams * vocab_size, divide it by num_beams to get vocabulary index and beam index\n",
        "        top_token_beam_indices = torch.div(top_token_indices, vocab_size,\n",
        "                                           rounding_mode=\"floor\")  # from which beam the top-token was retrieved from\n",
        "        top_token_indices = top_token_indices % vocab_size  # the index of top-token in the vocabulary\n",
        "\n",
        "        # TODO: Run beam_manager.process and save the results in unfinished_beam_scores, unfinished_token_indices and unfinished_beam_indices\n",
        "        unfinished_beam_outputs = beam_manager.process(input_ids, top_token_scores, top_token_indices, top_token_beam_indices)\n",
        "        unfinished_beam_scores = unfinished_beam_outputs[\"unfinished_scores\"]\n",
        "        unfinished_token_indices = unfinished_beam_outputs[\"unfinished_token_indices\"]\n",
        "        unfinished_beam_indices = unfinished_beam_outputs[\"unfinished_beam_indices\"]\n",
        "\n",
        "        # --- Prepare input_ids for next time step --- #\n",
        "        # TODO: index input_ids with the unfinished beam indices\n",
        "        # NOTE: input_ids should be (batch_size * num_beams, seq_len)\n",
        "        input_ids = input_ids[unfinished_beam_indices]\n",
        "\n",
        "        # TODO: concatenate the unfinished_token_indices to the input_ids\n",
        "        input_ids = torch.cat([input_ids, unfinished_token_indices.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "        # --- Do not change below --- #\n",
        "        # update the model_kwargs according to the concatenated input_ids\n",
        "        model_kwargs = model._update_model_kwargs_for_generation(\n",
        "            model_outputs, model_kwargs, is_encoder_decoder=False\n",
        "        )\n",
        "        if model_kwargs[\"past_key_values\"] is not None:\n",
        "            model_kwargs[\"past_key_values\"] = model._temporary_reorder_cache(\n",
        "                model_kwargs[\"past_key_values\"], unfinished_beam_indices,\n",
        "            )\n",
        "        # --- Do not change above --- #\n",
        "\n",
        "        # if unfinished input_ids reach the max seq length, exit the loop\n",
        "        if input_ids.size(-1) == max_length:\n",
        "            break\n",
        "\n",
        "    # TODO: Run beam_manager.finalize to get the best_input_ids and best_scores, among all finished / unfinished beams\n",
        "    best_input_ids, best_scores = beam_manager.finalize(input_ids, unfinished_beam_scores)\n",
        "\n",
        "    # if len(best_input_ids) < max_length, pad them to the max length\n",
        "    for batch_idx, sample_input_ids in enumerate(best_input_ids):\n",
        "        if sample_input_ids.size(-1) < max_length:\n",
        "            pad_tensor = torch.LongTensor([pad_token_id] * (max_length - sample_input_ids.size(-1))).to(device)\n",
        "\n",
        "            # TODO: pad best_input_ids with pad_tensor\n",
        "            best_input_ids[batch_idx] = torch.cat([sample_input_ids, pad_tensor], dim=-1)\n",
        "\n",
        "    # TODO: transform best_input_ids (which is currently a list of tensors) into a tensor of size (batch_idx, max_seq_length)\n",
        "    # Hint: use torch.stack\n",
        "    best_input_ids = torch.stack(best_input_ids, dim=0)\n",
        "\n",
        "    return tokenizer.batch_decode(best_input_ids, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Az-Q7QlywzZ"
      },
      "source": [
        "### Sanity check for debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "fuvQmmYWS3FX"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"This restaurant is awesome. I've been here a few\",\n",
              " 'My dog is cute and I love it.\\n\\nRated 5 out of',\n",
              " 'Today is sunny. The sun is shining. The']"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sents = [\n",
        "    \"This restaurant is awesome.\",\n",
        "    \"My dog is cute and I love it.\",\n",
        "    \"Today is sunny.\",\n",
        "]\n",
        "\n",
        "beam_search(sents, num_beams=5, max_length=15)\n",
        "\n",
        "# expected output: [\"This restaurant is awesome. I've been here a few\", 'My dog is cute and I love it.\\n\\nRated 5 out of', 'Today is sunny. The sun is shining. The']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYkg7i5ayyy_"
      },
      "source": [
        "### 2.2 Evaluate Beam Search!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "HGSf-kEUS--e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4ac3e8c7ef6421fbe10c29c4efa508f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "461afaa4f27a427b99b34b974345fe67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/13 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam Search\n",
            "perplexity = 2.05\n",
            "fluency = 0.82\n",
            "diversity = 0.07, 0.13, 0.17\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# If your implementation is efficient enough, the following code will run in no longer than 3 minutes with device = gpu.\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "prompts = [item['prompt'] for item in test_data][:100]\n",
        "MAX_LEN = 100\n",
        "NUM_BEAMS = 5\n",
        "BATCH_SIZE = 5\n",
        "\n",
        "generations = []\n",
        "for batch_start_idx in tqdm(range(0, len(prompts), BATCH_SIZE)):\n",
        "  batched_prompts = prompts[batch_start_idx: batch_start_idx + BATCH_SIZE]\n",
        "  batched_generations = beam_search(batched_prompts, NUM_BEAMS, MAX_LEN)\n",
        "\n",
        "  # remove prompt from generation\n",
        "  batched_generations = [generation[len(prompt):] for prompt, generation in zip(batched_prompts, batched_generations)]\n",
        "\n",
        "  generations += batched_generations\n",
        "\n",
        "evaluate(generations, 'Beam Search')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "27zicApAtc7Y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt 0\n",
            "The soccer game was tied 3 to 3 and there was a minute left to play.\n",
            "Generation 0\n",
            "\n",
            "\n",
            "\"I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not going to lie, I'm not\n",
            "---------\n",
            "\n",
            "Prompt 1\n",
            "Molly loves popcorn.\n",
            "Generation 1\n",
            "\n",
            "\n",
            "\"I love popcorn,\" she said. \"I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn. I love popcorn.\n",
            "---------\n",
            "\n",
            "Prompt 2\n",
            "Tim rented a car to visit his ill mother.\n",
            "Generation 2\n",
            "\n",
            "\n",
            "\"It was a very nice day,\" he said.\n",
            "\n",
            "\"It was a very nice day. It was a very nice day. It was a very nice day. It was a very nice day. It was a very nice day. It was a very nice day. It was a very nice day. It was a very nice day. It was a very nice day. It was a very\n",
            "---------\n",
            "\n",
            "Prompt 3\n",
            "Max had been dating Maddie for three Year's.\n",
            "Generation 3\n",
            "\n",
            "\n",
            "Maddie had been dating Maddie for three Year's.\n",
            "\n",
            "Maddie had been dating Maddie for three Year's.\n",
            "\n",
            "Maddie had been dating Maddie for three Year's.\n",
            "\n",
            "Maddie had been dating Maddie for three Year's.\n",
            "\n",
            "Maddie had been dating Maddie for three Year's.\n",
            "\n",
            "Maddie had been dating\n",
            "---------\n",
            "\n",
            "Prompt 4\n",
            "I'm waiting for a payment to come through the mail.\n",
            "Generation 4\n",
            " I'm waiting for a payment to come through the mail. I'm waiting for a payment to come through the mail. I'm waiting for a payment to come through the mail. I'm waiting for a payment to come through the mail. I'm waiting for a payment to come through the mail. I'm waiting for a payment to come through the mail. I'm waiting for a payment to come through the mail\n",
            "---------\n",
            "\n",
            "Prompt 5\n",
            "Kennan was born with a proprioceptive disorder.\n",
            "Generation 5\n",
            "\n",
            "\n",
            "He was diagnosed with Parkinson's disease in the early 1970s.\n",
            "\n",
            "He was diagnosed with Parkinson's disease in the early 1970s.\n",
            "\n",
            "He was diagnosed with Parkinson's disease in the early 1970s.\n",
            "\n",
            "He was diagnosed with Parkinson's disease in the early 1970s.\n",
            "\n",
            "He was diagnosed with Parkinson's disease in the early 1970s.\n",
            "\n",
            "He was diagnosed with Parkinson's\n",
            "---------\n",
            "\n",
            "Prompt 6\n",
            "Rich had worked in the woods for Years.\n",
            "Generation 6\n",
            "\n",
            "\n",
            "\"I'm not going to lie to you,\" he said. \"I'm not going to lie to you. I'm not going to lie to you. I'm not going to lie to you. I'm not going to lie to you. I'm not going to lie to you. I'm not going to lie to you. I'm not going to lie to you. I'm not going\n",
            "---------\n",
            "\n",
            "Prompt 7\n",
            "Tanya wanted to see her sister.\n",
            "Generation 7\n",
            "\n",
            "\n",
            "\"I don't know what you're talking about,\" she said.\n",
            "\n",
            "\"I don't know what you're talking about,\" she said.\n",
            "\n",
            "\"I don't know what you're talking about,\" she said.\n",
            "\n",
            "\"I don't know what you're talking about,\" she said.\n",
            "\n",
            "\"I don't know what you're talking about,\" she said.\n",
            "\n",
            "\"\n",
            "---------\n",
            "\n",
            "Prompt 8\n",
            "One afternoon Sallie took her dog, Gibby, to the dog park.\n",
            "Generation 8\n",
            "\n",
            "\n",
            "\"He was so happy,\" Sallie said. \"He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy. He was so happy\n",
            "---------\n",
            "\n",
            "Prompt 9\n",
            "Omar was walking his dog in a new neighborhood.\n",
            "Generation 9\n",
            "\n",
            "\n",
            "\"I was like, 'What the hell is going on?'\" he said. \"I was like, 'What the hell is going on?'\n",
            "\n",
            "\"I was like, 'What the hell is going on?' I was like, 'What the hell is going on?' I was like, 'What the hell is going on?' I was like, 'What the hell is going on?' I\n",
            "---------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# print first 10 generations\n",
        "\n",
        "sampled_prompts = prompts[:10]\n",
        "sampled_generations = generations[:10]\n",
        "\n",
        "for idx, (prompt, generation) in enumerate(zip(sampled_prompts, sampled_generations)):\n",
        "  print(f\"Prompt {idx}\")\n",
        "  print(prompt)\n",
        "  print(f\"Generation {idx}\")\n",
        "  print(generation)\n",
        "  print(\"---------\")\n",
        "  print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
