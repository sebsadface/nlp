{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 4: Decoding Algorithms (Section 3: NeuroLogic)\n",
        "  \n",
        "## Section 0: Setup\n",
        "\n",
        "Please run all the code blocks in this section. You don't need to implement or change anything here."
      ],
      "metadata": {
        "id": "zRXwnlArQ6aW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lSg7SGaR5fM"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"set device and random seeds\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper functions are given to you.\n",
        "######################################################\n",
        "\n",
        "import torch\n",
        "import random\n",
        "\n",
        "def set_seed(seed=19260817):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()"
      ],
      "metadata": {
        "id": "61_kgWLUoed9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.1 Load dataset"
      ],
      "metadata": {
        "id": "29d155b4RBmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"load datasets\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('Ximing/ROCStories')\n",
        "train_data, dev_data, test_data = dataset['train'], dataset['validation'], dataset['test']\n",
        "\n",
        "print(train_data[0])"
      ],
      "metadata": {
        "id": "kPcsD3_Gqm7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.2 Define Evaluation models and metrics"
      ],
      "metadata": {
        "id": "8bzYUUZrREuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"prepare evaluation\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as f\n",
        "from evaluate import load\n",
        "\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'device: {device}')\n",
        "\n",
        "perplexity_scorer = load(\"perplexity\", module_type=\"metric\")\n",
        "cola_model_name = \"textattack/roberta-base-CoLA\"\n",
        "cola_tokenizer = RobertaTokenizer.from_pretrained(cola_model_name)\n",
        "cola_model = RobertaForSequenceClassification.from_pretrained(cola_model_name).to(device)\n",
        "\n",
        "def batchify(data, batch_size):\n",
        "    assert batch_size > 0\n",
        "\n",
        "    batch = []\n",
        "    for item in data:\n",
        "        # Yield next batch\n",
        "        if len(batch) == batch_size:\n",
        "            yield batch\n",
        "            batch = []\n",
        "\n",
        "        batch.append(item)\n",
        "\n",
        "    # Yield last un-filled batch\n",
        "    if len(batch) != 0:\n",
        "        yield batch"
      ],
      "metadata": {
        "id": "gmMd57VjSEZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"set up evaluation metric\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "def compute_perplexity(texts, model='gpt2', batch_size=8):\n",
        "    score = perplexity_scorer.compute(predictions=texts, add_start_token=False, batch_size=batch_size, model_id=model)\n",
        "    return score['mean_perplexity']\n",
        "\n",
        "def compute_fluency(texts, batch_size=8):\n",
        "  scores = []\n",
        "  for b_texts in batchify(texts, batch_size):\n",
        "    inputs = cola_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "      logits = cola_model(**inputs).logits\n",
        "      probs = logits.softmax(dim=-1)\n",
        "      scores.extend(probs[:, 1].tolist())\n",
        "  return sum(scores) / len(scores)\n",
        "\n",
        "def compute_diversity(texts):\n",
        "    unigrams, bigrams, trigrams = set(), set(), set()\n",
        "    total_words = 0\n",
        "    for gen in texts:\n",
        "        o = gen.split(' ')\n",
        "        total_words += len(o)\n",
        "        unigrams.update(o)\n",
        "        for i in range(len(o) - 1):\n",
        "            bigrams.add(o[i] + '_' + o[i + 1])\n",
        "        for i in range(len(o) - 2):\n",
        "            trigrams.add(o[i] + '_' + o[i + 1] + '_' + o[i + 2])\n",
        "    return len(unigrams) / total_words, len(bigrams) / total_words, len(trigrams) / total_words\n",
        "\n",
        "\n",
        "test_sents = [\"This restaurant is awesome\", \"My dog is cute and I love it.\", \"Today is sunny.\"]\n",
        "print(compute_perplexity(test_sents))\n",
        "print(compute_fluency(test_sents))\n",
        "print(compute_diversity(test_sents))"
      ],
      "metadata": {
        "id": "7t97XqNh2Xu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Neurologic Implementation"
      ],
      "metadata": {
        "id": "uv0P_mt1RWh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configurations: load model and tokenizer"
      ],
      "metadata": {
        "id": "jPbvq4LnRd_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"load model and tokenizer\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GenerationConfig\n",
        "\n",
        "model_name = 'gpt2'\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "model.eval()\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name, pad_token=\"<|endoftext|>\")\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "eos_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "78dmI0HW4P-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper classes: `Word`, `ConstrainedHypothesis` and `ConstrainedCandidate`"
      ],
      "metadata": {
        "id": "uBB6FYDrPZjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "class Word:\n",
        "    def __init__(self, idx: int, word_token: int):\n",
        "        \"\"\"\n",
        "        :param idx: index id for this keyword\n",
        "        :param word_token: token of the keyword, we assume single-token constraint for this homework\n",
        "        \"\"\"\n",
        "        self.idx = idx\n",
        "        self.token = word_token\n",
        "        self.satisfied = False\n",
        "\n",
        "    def __str__(self):\n",
        "        return f'word(id={self.idx}, token={self.token}, satisfy={self.satisfied})'\n",
        "\n",
        "    def advance(self, next_token: int):\n",
        "        \"\"\"\n",
        "        :param next_token: selected token at current time step\n",
        "        \"\"\"\n",
        "        if next_token == self.token:\n",
        "            self.satisfied = True\n",
        "\n",
        "\n",
        "class ConstrainedHypothesis:\n",
        "    def __init__(self, keyword_list: List[int]):\n",
        "        \"\"\"\n",
        "        :param keyword_list: list of tokenized keywords\n",
        "        \"\"\"\n",
        "        self.words = []\n",
        "        for idx, word_token in enumerate(keyword_list):\n",
        "            self.words.append(Word(idx=idx, word_token=word_token))\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return '\\t'.join([str(w) for w in self.words])\n",
        "\n",
        "    def num_satisfied_keywords(self) -> int:\n",
        "        \"\"\"\n",
        "        :return: number of satisfied keywords\n",
        "        \"\"\"\n",
        "        # return how many keywords are satisfied\n",
        "        return sum([int(w.satisfied) for w in self.words])\n",
        "\n",
        "    def get_satisfied_keywords_idx(self) -> Tuple[int]:\n",
        "        \"\"\"\n",
        "        :return: the index ids of the satisfied keywords, from low to high\n",
        "        \"\"\"\n",
        "        # traverse through self.words, add word.idx to the return tuple if it's satisfied,\n",
        "        #  notice that the index ids should be sorted from low to high\n",
        "        satisfied_keywords_idx = [w.idx for w in self.words if w.satisfied]\n",
        "        satisfied_keywords_idx = tuple(sorted(satisfied_keywords_idx))\n",
        "        return satisfied_keywords_idx\n",
        "\n",
        "    def get_unsatisfied_words(self) -> List[int]:\n",
        "        \"\"\"\n",
        "        :return: the token of keywords that are not satisfied\n",
        "        \"\"\"\n",
        "        return [w.token for w in self.words if not w.satisfied]\n",
        "\n",
        "    def advance(self, next_token: int) -> 'ConstrainedHypothesis':\n",
        "        \"\"\"\n",
        "        :param next_token: selected token at current time step\n",
        "        :return: a new ConstrainedHypothesis object with updated state based on the next token\n",
        "        \"\"\"\n",
        "        obj = copy.deepcopy(self)\n",
        "        # update the keyword satisfaction state of obj based on the next token\n",
        "        for word in obj.words:\n",
        "            if word.satisfied:\n",
        "                continue\n",
        "            word.advance(next_token)\n",
        "        return obj\n",
        "\n",
        "\n",
        "class ConstrainedCandidate:\n",
        "    \"\"\"\n",
        "    :param row: The row (beam index) in the scores matrix.\n",
        "    :param col: The column (token id) in the scores matrix.\n",
        "    :param score: the associated accumulated score.\n",
        "    :param hypothesis: The ConstrainedHypothesis containing constraint information.\n",
        "    \"\"\"\n",
        "\n",
        "    __slots__ = ('row', 'col', 'score', 'hypothesis')\n",
        "\n",
        "    def __init__(self, row: int, col: int, score: float, hypothesis: ConstrainedHypothesis):\n",
        "        self.row = row\n",
        "        self.col = col\n",
        "        self.score = score\n",
        "        self.hypothesis = hypothesis\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash((self.row, self.col))\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return self.row == other.row and self.col == other.col\n",
        "\n",
        "    def __str__(self):\n",
        "        return '[{}, {}, {}, {}]'.format(self.row, self.col, self.score, str(self.hypothesis))\n",
        "\n",
        "\n",
        "def initialize_constraint(keyword_lists: List[List[int]], beam_size: int) -> List[ConstrainedHypothesis]:\n",
        "    \"\"\"\n",
        "    :param keyword_lists: list of tokenized keyword list in a batch\n",
        "    :param beam_size: beam size\n",
        "    :return: list of initialized ConstrainedHypothesis objected with shape (batch_size * beam_size,)\n",
        "    \"\"\"\n",
        "    batch_size = len(keyword_lists)\n",
        "    constraints_list = [None] * (batch_size * beam_size)\n",
        "    for i, word_list in enumerate(keyword_lists):\n",
        "        hyp = ConstrainedHypothesis(word_list)\n",
        "        start_idx = i * beam_size\n",
        "        constraints_list[start_idx:start_idx + beam_size] = [copy.deepcopy(hyp) for _ in range(beam_size)]\n",
        "    return constraints_list"
      ],
      "metadata": {
        "id": "5hwd8J4XOqLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper classes: `BeamHypothesisList` and `BeamManager`"
      ],
      "metadata": {
        "id": "Fq9lOjrwRSrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "@dataclass\n",
        "class BeamHypothesis:\n",
        "    def __init__(self, input_ids: torch.LongTensor, score: torch.FloatTensor, num_satisfied_keywords: int):\n",
        "        self.input_ids: torch.LongTensor = input_ids  # a single token sequence of size (seq_len,)\n",
        "        self.score: torch.FloatTensor = score  # a scalar score for the token sequence\n",
        "        self.num_satisfied_keywords: int = num_satisfied_keywords\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"BeamHypothesis(input_ids: {self.input_ids}, score: {self.score}, num_satisfied_keywords: {self.num_satisfied_keywords})\"\n",
        "\n",
        "\n",
        "class BeamHypothesisList:\n",
        "    def __init__(self, num_beams: int):\n",
        "        self.beam_hypotheses: List[BeamHypothesis] = []  # list of beam_hypothesis\n",
        "        self.num_beams: int = num_beams\n",
        "\n",
        "        self.worst_score = {}  # worst beam score of hypotheses with certain num_satisfied_keywords in self.beam_hypotheses\n",
        "\n",
        "    def add(self, new_input_ids: torch.LongTensor, sum_logprobs: float, num_satisfied_keywords: int):\n",
        "        \"\"\"\n",
        "        :param new_input_ids: new token sequence of size (1, seq_len)\n",
        "        :param sum_logprobs: sum of log probabilities of tokens in new_input_ids\n",
        "        :param num_satisfied_keywords: number of satisfied constraints\n",
        "        Given a new hypothesis (new_input_ids) and its corresponding sum_logprobs and num_satisfied_keywords,\n",
        "        update self.beam_hypotheses with a finished hypothesis.\n",
        "        (1) If self.beam_hypotheses contains hypotheses with the same number of satisfied keywords\n",
        "            and the new_input_ids has higher score than the worst score within the hypothesis group with the same number of satisfied keywords\n",
        "            we replace the worst scoring hypothesis within this group with the new hypothesis.\n",
        "        (2) If self.beam_hypotheses doesn't contain hypotheses with the same number of satisfied keywords\n",
        "            and the number of satisfied keywords is greater than the lowest number of satisfied keywords in self.beam_hypotheses,\n",
        "            we replace the worst scoring hypothesis within the group with the lowest number of satisfied keywords with the new hypothesis.\n",
        "        (3) Otherwise, we do not make any change to self.beam_hypotheses.\n",
        "        \"\"\"\n",
        "        # For score, we compute average token log probability\n",
        "        score = sum_logprobs / new_input_ids.size(-1)\n",
        "\n",
        "        # add the new hypothesis if we still have vacant beams\n",
        "        if len(self.beam_hypotheses) < self.num_beams:\n",
        "            # initialize the new_beam_hypothesis using new_input_ids and score\n",
        "            new_beam_hypothesis: BeamHypothesis = BeamHypothesis(input_ids=new_input_ids, score=score,\n",
        "                                                                 num_satisfied_keywords=num_satisfied_keywords)\n",
        "\n",
        "            # add new_beam_hypothesis to beam_hypotheses\n",
        "            self.beam_hypotheses.append(new_beam_hypothesis)\n",
        "\n",
        "            # keep track of the worst score among hypotheses with certain number of satisfied keywords\n",
        "            if num_satisfied_keywords not in self.worst_score:\n",
        "                self.worst_score[num_satisfied_keywords] = float(score)\n",
        "            else:\n",
        "                self.worst_score[num_satisfied_keywords] = min(self.worst_score[num_satisfied_keywords], float(score))\n",
        "\n",
        "        # new_input_ids has higher score than the worst score within the hypothesis group with the same number of satisfied keywords\n",
        "        elif num_satisfied_keywords in self.worst_score and score > self.worst_score[num_satisfied_keywords]:\n",
        "            # find the group of hypotheses with the same number of satisfied keywords\n",
        "            same_keywords_group = [hyp for hyp in self.beam_hypotheses if hyp.num_satisfied_keywords == num_satisfied_keywords]\n",
        "            # remove the worst hypothesis, the one with the lowest score in this group\n",
        "            worst_hypothesis: BeamHypothesis = min(same_keywords_group, key=lambda hyp: hyp.score)\n",
        "            self.beam_hypotheses.remove(worst_hypothesis)\n",
        "\n",
        "            # add new hypothesis\n",
        "            # initialize the new_beam_hypothesis using new_input_ids, score and num_satisfied_keywords\n",
        "            new_beam_hypothesis = BeamHypothesis(input_ids=new_input_ids, score=score,\n",
        "                                                 num_satisfied_keywords=num_satisfied_keywords)\n",
        "\n",
        "            # add new_beam_hypothesis to beam_hypotheses\n",
        "            self.beam_hypotheses.append(new_beam_hypothesis)\n",
        "\n",
        "            # update the worst score for group with num_satisfied_keywords\n",
        "            same_keywords_group = [hyp for hyp in self.beam_hypotheses if hyp.num_satisfied_keywords == num_satisfied_keywords]\n",
        "            worst_score = min(float(hyp.score) for hyp in same_keywords_group)\n",
        "            self.worst_score[num_satisfied_keywords] = worst_score\n",
        "\n",
        "        # the number of satisfied keywords is greater than the lowest number of satisfied keywords\n",
        "        elif num_satisfied_keywords not in self.worst_score and num_satisfied_keywords > min(self.worst_score.keys()):\n",
        "            # find the group of hypotheses with the lowest number of satisfied keywords\n",
        "            lowest_keywords_group = [hyp for hyp in self.beam_hypotheses if hyp.num_satisfied_keywords == min(self.worst_score.keys())]\n",
        "            # remove the worst hypothesis, the one with the lowest score in this group\n",
        "            worst_hypothesis: BeamHypothesis = min(lowest_keywords_group, key=lambda hyp: hyp.score)\n",
        "            self.beam_hypotheses.remove(worst_hypothesis)\n",
        "\n",
        "            # add new hypothesis\n",
        "            # initialize the new_beam_hypothesis using new_input_ids, score and num_satisfied_keywords\n",
        "            new_beam_hypothesis = BeamHypothesis(input_ids=new_input_ids, score=score,\n",
        "                                                 num_satisfied_keywords=num_satisfied_keywords)\n",
        "\n",
        "            # add new_beam_hypothesis to beam_hypotheses\n",
        "            self.beam_hypotheses.append(new_beam_hypothesis)\n",
        "\n",
        "            # update the worst score for group with num_satisfied_keywords\n",
        "            self.worst_score[num_satisfied_keywords] = float(score)\n",
        "            # update the worst score for group with the lowest num_satisfied_keywords\n",
        "            lowest_keywords_group = [hyp for hyp in self.beam_hypotheses if hyp.num_satisfied_keywords == min(self.worst_score.keys())]\n",
        "            worst_score = min(float(hyp.score) for hyp in lowest_keywords_group)\n",
        "            self.worst_score[min(self.worst_score.keys())] = worst_score\n",
        "\n",
        "        # sanity check\n",
        "        assert len(self.beam_hypotheses) <= self.num_beams"
      ],
      "metadata": {
        "id": "Ut9kXr0LRf4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "class BeamManager:\n",
        "    def __init__(self, batch_size: int, num_beams: int):\n",
        "        self.finished_beam_hypotheses_list = [BeamHypothesisList(num_beams) for _ in range(batch_size)]\n",
        "        self.batch_size = batch_size\n",
        "        self.num_beams = num_beams\n",
        "\n",
        "    def process(self,\n",
        "                input_ids: torch.LongTensor,\n",
        "                top_token_scores: torch.FloatTensor,\n",
        "                top_token_indices: torch.LongTensor,\n",
        "                top_token_beam_indices: torch.LongTensor,\n",
        "                constraint_hypotheses: List[ConstrainedHypothesis],\n",
        "                ):\n",
        "        \"\"\"\n",
        "        :param input_ids: (batch_size * num_beams, current_seq_length), the input_ids that were used to compute top_tokens\n",
        "        :param top_token_scores: (batch_size, 2 * num_beams), representing the score of each top token\n",
        "        :param top_token_indices: (batch_size, 2 * num_beams), representing each token's index (in vocabulary) of the top tokens\n",
        "        :param top_token_beam_indices: (batch_size, 2 * num_beams), representing each token's corresponding beam index of the top tokens\n",
        "        :param constraint_hypotheses: (batch_size * 2 * num_beams), the constraint state of each top token\n",
        "\n",
        "        Note: the input arguments `top_token_*` for each sample in batch are sorted from the largest score to the smallest score.\n",
        "        For example, if batch_size = 2 and num_beams = 3, then each of these values denote\n",
        "        top_token_indices[1, 2]: what is the third-best next token for the second sample in the batch?\n",
        "        top_token_scores[0, 1]: what is the score of the second-best next token for the first sample in the batch?\n",
        "        top_token_beam_indices[0, 1]: which beam did we use to generate the second-best next token for the first sample in the batch?\n",
        "\n",
        "        In this function, for each of the top-(2 * num_beams) tokens, we do the following:\n",
        "        (1) If the top token is EOS token:\n",
        "            This means that this hypothesis is done. Therefore, we save the hypothesis so-far to self.finished_beam_hypotheses_list.\n",
        "        (2) If the top token is not EOS token\n",
        "            We have to keep searching with this hypothesis. Therefore, we prepare the hypothesis for next time step.\n",
        "\n",
        "        Returns a dictionary, where\n",
        "        \"unfinished_scores\": size (batch_size * num_beams,), the score of the unfinished beams\n",
        "        \"unfinished_token_indices\": size (batch_size * num_beams,), the index of the last token in the unfinished beams\n",
        "        \"unfinished_beam_indices\": the index of the beam that was used to generate the new unfinished beam\n",
        "        \"\"\"\n",
        "        device = top_token_scores.device\n",
        "\n",
        "        # Initialize unfinished_token_*, which we will return for the next time step.\n",
        "        unfinished_scores = torch.zeros((self.batch_size, self.num_beams), dtype=top_token_scores.dtype).to(device)  # score of the unfinished beams\n",
        "        unfinished_token_indices = torch.zeros((self.batch_size, self.num_beams), dtype=top_token_indices.dtype).to(\n",
        "            device)  # index of the last token of the unfinished beams\n",
        "        unfinished_token_beam_indices = torch.zeros((self.batch_size, self.num_beams), dtype=top_token_beam_indices.dtype).to(\n",
        "            device)  # index of the unfinished beam in the batch\n",
        "        unfinished_constraint_hypotheses = [None] * (self.batch_size * self.num_beams)\n",
        "\n",
        "        # Loop over the batch\n",
        "        for batch_idx in range(self.batch_size):\n",
        "            # get sample_beam_hypothesis_list: the finished_beam_hypothesis_list for this sample in the batch\n",
        "            sample_beam_hypothesis_list: BeamHypothesisList = self.finished_beam_hypotheses_list[batch_idx]\n",
        "\n",
        "            # get the top_token_scores, top_token_indices, top_token_beam_indices, constraint_hypotheses for this sample in the batch\n",
        "            # NOTE: size of sample_top_token_*: (2 * num_beams,)\n",
        "            sample_top_token_scores = top_token_scores[batch_idx]\n",
        "            sample_top_token_indices = top_token_indices[batch_idx]\n",
        "            sample_top_token_beam_indices = top_token_beam_indices[batch_idx]\n",
        "            sample_const_hypos = constraint_hypotheses[batch_idx * (2 * self.num_beams): (batch_idx + 1) * (2 * self.num_beams)]\n",
        "\n",
        "            # Loop over all top tokens\n",
        "            sample_beam_idx = 0\n",
        "            for top_token_score, top_token_index, top_token_beam_index, const_hypo in zip(\n",
        "                    sample_top_token_scores, sample_top_token_indices, sample_top_token_beam_indices, sample_const_hypos\n",
        "            ):\n",
        "                # Note that top_token_beam_indices only denotes the index of the beam in each sample.\n",
        "                # We transform this into `beam_idx_in_batch`, we denote the index of the beam among all (batch_size * num_beams) beams in the batch.\n",
        "                beam_idx_in_batch = batch_idx * self.num_beams + top_token_beam_index\n",
        "\n",
        "                # if top_token == EOS, we add the generation so-far to the beam_hypotheses_list\n",
        "                if top_token_index.item() == eos_token_id:\n",
        "                    # among the (batch_size * num_beams) input_ids, find the input_ids that correspond to this top_token\n",
        "                    # NOTE: the size of new_input_ids: (seq_len,)\n",
        "                    new_input_ids = input_ids[beam_idx_in_batch]\n",
        "\n",
        "                    # add the new beam to sample_beam_hypothesis_list\n",
        "                    sample_beam_hypothesis_list.add(\n",
        "                        new_input_ids,\n",
        "                        top_token_score,\n",
        "                        const_hypo.num_satisfied_keywords(),\n",
        "                    )\n",
        "\n",
        "                # if top_token =/= EOS, we aggregate them for next time step.\n",
        "                else:\n",
        "                    # store the score, token_index, beam_idx_in_batch to the unfinished_scores, unfinished_token_indices, unfinished_token_beam_indices\n",
        "                    unfinished_scores[batch_idx, sample_beam_idx] = top_token_score\n",
        "                    unfinished_token_indices[batch_idx, sample_beam_idx] = top_token_index\n",
        "                    unfinished_token_beam_indices[batch_idx, sample_beam_idx] = beam_idx_in_batch\n",
        "                    unfinished_constraint_hypotheses[batch_idx * self.num_beams + sample_beam_idx] = const_hypo\n",
        "\n",
        "                    sample_beam_idx += 1\n",
        "\n",
        "                # once we have `num_beams` number of new beams, we don't have to add anymore.\n",
        "                if sample_beam_idx == self.num_beams:\n",
        "                    break\n",
        "\n",
        "        # return the dictionary of unfinished_scores, unfinished_token_indices, unfinished_beam_indices, unfinished_constraint_hypotheses\n",
        "        # Make sure to change the size of each tensor to (batch_size * num_beams,)\n",
        "        return {\n",
        "            \"unfinished_scores\": unfinished_scores.view(-1),  # (batch_size * num_beams,)\n",
        "            \"unfinished_token_indices\": unfinished_token_indices.view(-1),  # (batch_size * num_beams,)\n",
        "            \"unfinished_beam_indices\": unfinished_token_beam_indices.view(-1),  # (batch_size * num_beams,)\n",
        "            \"unfinished_constraint_hypotheses\": unfinished_constraint_hypotheses, # (batch_size * num_beams,)\n",
        "        }\n",
        "\n",
        "    def finalize(\n",
        "            self,\n",
        "            input_ids: torch.LongTensor,\n",
        "            beam_scores: torch.FloatTensor,\n",
        "            constraint_hypotheses: List[ConstrainedHypothesis],\n",
        "    ) -> Tuple[List[torch.LongTensor], List[torch.FloatTensor]]:\n",
        "        \"\"\"\n",
        "        :param input_ids: (batch_idx * num_beams, max_length), input_ids of unfinished beams\n",
        "        :param beam_scores: (batch_idx * num_beams,), scores of unfinished beams\n",
        "        :param constraint_hypotheses: (batch_size * num_beams, ), the constraint state of unfinished beams\n",
        "\n",
        "        Get the final best beams, among\n",
        "        (1) unfinished beams, for which we get the input_ids, beam_scores and constraint_hypotheses as arguments\n",
        "        (2) finished beams, which we store in self.batch_beam_hypothesis_list\n",
        "        Returns a tuple of two lists, where\n",
        "        - tuple[0] is the list of the input_ids of the best beams (length: batch_idx)\n",
        "        - tuple[1] is the list of the scores of the best beams (length: batch_idx\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. Add all unfinished beam hypotheses to self.finished_beam_hypotheses_list\n",
        "        for batch_idx in range(self.batch_size):\n",
        "            # get sample_beam_hypothesis_list: the finished_beam_hypothesis_list for this sample in the batch\n",
        "            sample_beam_hypothesis_list: BeamHypothesisList = self.finished_beam_hypotheses_list[batch_idx]\n",
        "\n",
        "            for sample_beam_idx in range(self.num_beams):\n",
        "                # get beam_idx_in_batch: index of the beam in all `batch_size * num_beams` beams in the batch\n",
        "                beam_idx_in_batch = batch_idx * self.num_beams + sample_beam_idx\n",
        "\n",
        "                # get the input_id for this beam, using `beam_idx_in_batch`\n",
        "                # NOTE: the size of new_input_ids: (seq_len,)\n",
        "                new_input_ids = input_ids[beam_idx_in_batch]\n",
        "\n",
        "                # get the score of this beam, using `beam_idx_in_batch`\n",
        "                # NOTE: beam_score should be a scalar\n",
        "                beam_score = beam_scores[beam_idx_in_batch].item()\n",
        "\n",
        "                # get the number of satisfied keywords of this beam, using `constraint_hypotheses`\n",
        "                num_satisfied_keywords = constraint_hypotheses[beam_idx_in_batch].num_satisfied_keywords()\n",
        "\n",
        "                # add the new hypothesis to sample_beam_hypothesis_list\n",
        "                sample_beam_hypothesis_list.add(new_input_ids, beam_score, num_satisfied_keywords)\n",
        "\n",
        "        # 2. Select the best hypothesis from each beam_hypothesis_list\n",
        "        best_input_ids = []\n",
        "        best_scores = []\n",
        "        for batch_idx in range(self.batch_size):\n",
        "            # get sample_beam_hypothesis_list: the finished_beam_hypothesis_list for this sample in the batch\n",
        "            sample_beam_hypothesis_list: BeamHypothesisList = self.finished_beam_hypotheses_list[batch_idx]\n",
        "\n",
        "            # get the group of hypotheses with the highest number of satisfied keywords\n",
        "            max_num_satisfied_keywords = max([hyp.num_satisfied_keywords for hyp in sample_beam_hypothesis_list.beam_hypotheses])\n",
        "            max_satisfied_group = [hyp for hyp in sample_beam_hypothesis_list.beam_hypotheses if hyp.num_satisfied_keywords == max_num_satisfied_keywords]\n",
        "\n",
        "            # get best_hypothesis among this group (the one with the highest score)\n",
        "            best_hypothesis = max(max_satisfied_group, key=lambda hyp: hyp.score)\n",
        "\n",
        "            # save the input_ids and score of best_hypothesis\n",
        "            best_input_ids.append(best_hypothesis.input_ids)\n",
        "            best_scores.append(best_hypothesis.score)\n",
        "\n",
        "        return best_input_ids, best_scores"
      ],
      "metadata": {
        "id": "2HD5CoYBSvpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neurologic Search"
      ],
      "metadata": {
        "id": "hXle5lWRRVma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rerank_beam(new_scores: torch.FloatTensor,\n",
        "                top_token_beam_indices: torch.LongTensor,\n",
        "                top_token_indices: torch.LongTensor,\n",
        "                constraint_hypotheses: List[ConstrainedHypothesis]):\n",
        "    \"\"\"\n",
        "        :param num_beams: number of beams\n",
        "        :param new_scores: (batch_size, num_beams, vocab_size), accumulated score form previous beam_scores and the next_token_scores\n",
        "        :param top_token_indices: (batch_size, 2 * num_beams), representing each token's index (in vocabulary) of the top tokens\n",
        "        :param top_token_beam_indices: (batch_size, 2 * num_beams), representing each token's corresponding beam index of the top tokens\n",
        "        :param constraint_hypotheses: the list of constraint hypothesis objects. (length: (batch_size * num_beams,))\n",
        "    \"\"\"\n",
        "    batch_size, num_beams, vocab_size = new_scores.shape\n",
        "\n",
        "    # Initialize new_*, which we will return for the next time step.\n",
        "    new_top_token_beam_indices = torch.zeros((batch_size, 2 * num_beams), dtype=top_token_beam_indices.dtype).to(device)\n",
        "    new_top_token_indices = torch.zeros((batch_size, 2 * num_beams), dtype=top_token_indices.dtype).to(device)\n",
        "    new_top_token_scores = torch.zeros((batch_size, 2 * num_beams), dtype=new_scores.dtype).to(device)\n",
        "    new_constraint_hypotheses = [None] * (batch_size * (2 * num_beams))\n",
        "\n",
        "    for batch_idx in range(batch_size):\n",
        "        scores = new_scores[batch_idx]  # (num_beams, vocab_size)\n",
        "        best_beam_idx = top_token_beam_indices[batch_idx]  # (2 * num_beams,)\n",
        "        best_token_idx = top_token_indices[batch_idx]  # (2 * num_beams,)\n",
        "        const_hypos = constraint_hypotheses[batch_idx * num_beams: (batch_idx + 1) * num_beams]  # (num_beams,)\n",
        "\n",
        "        candidates = set()\n",
        "        for row, col in zip(best_beam_idx.tolist(), best_token_idx.tolist()):\n",
        "            # add ConstrainedCandidate with (row, col) from (best_beam_idx, best_token_idx) to candidates\n",
        "            # NOTE: you need to update corresponding constraint hypothesis to get a new constraint hypothesis\n",
        "            # to fill in the field of ConstrainedCandidate\n",
        "            new_hypo = const_hypos[row].advance(col)\n",
        "            candidate = ConstrainedCandidate(row, col, scores[row, col], new_hypo)\n",
        "            candidates.add(candidate)\n",
        "\n",
        "        for row in range(num_beams):\n",
        "            # for each previous beam (row),\n",
        "            #  (1) add ConstrainedCandidates with the next token (col) that would satisfy one more constraint to candidates\n",
        "            #  (2) add ConstrainedCandidate with the best next_token (col) to preserve candidates who satisfied constraints in the previous steps\n",
        "            const_cols = const_hypos[row].get_unsatisfied_words()\n",
        "            const_cols.append(torch.argmax(scores[row]).item())\n",
        "\n",
        "            for col in const_cols:\n",
        "                new_hypo = const_hypos[row].advance(col)\n",
        "                candidate = ConstrainedCandidate(row, col, scores[row, col], new_hypo)\n",
        "                candidates.add(candidate)\n",
        "\n",
        "        # group candidates by the index ids of the satisfied keywords (i.e. get_satisfied_keywords_idx())\n",
        "        idx_combs = set([x.hypothesis.get_satisfied_keywords_idx() for x in candidates])\n",
        "        grouped_candidates = [[x for x in candidates if x.hypothesis.get_satisfied_keywords_idx() == c] for c in idx_combs]\n",
        "        # sort candidates in each group by score, from high to low\n",
        "        grouped_sorted_candidates = [sorted(g, key=lambda x: x.score, reverse=True) for g in grouped_candidates]\n",
        "\n",
        "        # NOTE: get rid of candidate with -inf score, which is resulted by padding\n",
        "        grouped_sorted_candidates = [[c for c in g if c.score > -1e8] for g in grouped_sorted_candidates]\n",
        "\n",
        "        selected_candidates = []\n",
        "        # select the top candidates with highest score within each group to fill in selected_candidates\n",
        "        for top_n in range(max([len(g) for g in grouped_sorted_candidates])):\n",
        "            # get top_n items from all the groups with len(group) >= n\n",
        "            top_n_items = [g[top_n] for g in grouped_sorted_candidates if len(g) > top_n]\n",
        "            # sort top_n_items by score\n",
        "            sorted_top_n_items = sorted(top_n_items, key=lambda x: x.score, reverse=True)\n",
        "            # add sorted top_n_items to selected_candidates\n",
        "            selected_candidates.extend(sorted_top_n_items)\n",
        "\n",
        "        # return the top 2 * num_beams candidates as the hypotheses for next time step\n",
        "        selected_candidates = selected_candidates[:2 * num_beams]\n",
        "        for j, selected_candidate in enumerate(selected_candidates):\n",
        "            new_top_token_beam_indices[batch_idx, j] = selected_candidate.row\n",
        "            new_top_token_indices[batch_idx, j] = selected_candidate.col\n",
        "            new_top_token_scores[batch_idx, j] = selected_candidate.score\n",
        "            new_constraint_hypotheses[batch_idx * (2 * num_beams) + j] = selected_candidate.hypothesis\n",
        "\n",
        "    return new_top_token_beam_indices, new_top_token_indices, new_top_token_scores, new_constraint_hypotheses\n",
        "\n",
        "\n",
        "def neurologic_search(prompts: List[str], keyword_lists: List[List[str]], num_beams: int, max_length: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    :param prompts: list of prompt strings\n",
        "    :param keyword_lists: list of keyword list\n",
        "    :param num_beams: number of beams\n",
        "    :param max_length: max generation length\n",
        "    :return: list of generation, including both the original prompt and generation\n",
        "    \"\"\"\n",
        "    # encode the prompts using tokenizer (padding=True), to get input_ids and attention_mask\n",
        "    # Note: don't forget to push the encoded text to device.\n",
        "    input_encoding = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(device)\n",
        "    input_ids, attention_mask = input_encoding[\"input_ids\"], input_encoding[\"attention_mask\"]\n",
        "\n",
        "    tokenized_keywords = [list(map(lambda x: tokenizer.encode(f' {x}')[0], k_list)) for k_list in keyword_lists]\n",
        "    constraints = initialize_constraint(tokenized_keywords, num_beams)\n",
        "\n",
        "    if input_ids.size(-1) > max_length:\n",
        "      raise ValueError(\"Input ID is larger than max_length.\")\n",
        "\n",
        "    # --- Do not change below --- #\n",
        "    batch_size = input_ids.size(0)\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # initialize model_kwargs\n",
        "    model_kwargs = {'attention_mask': attention_mask}\n",
        "\n",
        "    # interleave input_ids according to num_beams.\n",
        "    # For example, input_ids for [\"Hi\", \"good\"] with num_beams=3 becomes [\"Hi\", \"Hi\", \"Hi\", \"good\", \"good\", \"good\"]\n",
        "    input_ids, model_kwargs = model._expand_inputs_for_generation(\n",
        "        input_ids=input_ids,\n",
        "        expand_size=num_beams,\n",
        "        is_encoder_decoder=False,\n",
        "        **model_kwargs,\n",
        "    )\n",
        "    # input_ids: tensor of size (batch_size * num_beams, seq_len)\n",
        "    # model_kwargs: a dictionary with single element 'attention_mask', sized (batch_size * num_beams, seq_len)\n",
        "    # --- Do not change above --- #\n",
        "\n",
        "    # initialize beam_manager\n",
        "    beam_manager = BeamManager(batch_size=batch_size, num_beams=num_beams)\n",
        "\n",
        "    # initialize unfinished_beam_scores, a tensor of size (batch_size, num_beams) with all elements = 0\n",
        "    unfinished_beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=device)\n",
        "\n",
        "    # For each sample in the batch, set all initial beam_score to -1e9, except for the first beam\n",
        "    unfinished_beam_scores[:, 1:] = -1e9\n",
        "    unfinished_beam_scores = unfinished_beam_scores.view(-1)  # (batch_size * num_beams,)\n",
        "\n",
        "    while True:\n",
        "        # --- Do not change below --- #\n",
        "        model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "        # --- Do not change above --- #\n",
        "\n",
        "        # run model forward pass with model_inputs as the input\n",
        "        # NOTE: we should set return_dict=True, output_attentions=False and output_hidden_states=False\n",
        "        model_outputs = model(\n",
        "            **model_inputs,\n",
        "            return_dict=True,\n",
        "            output_attentions=False,\n",
        "            output_hidden_states=False\n",
        "        )\n",
        "\n",
        "        # compute log_probs for next tokens given the prompt\n",
        "        # NOTE: size of next_token_scores: (batch_size * num_beams, vocab_size)\n",
        "        next_token_logits = model_outputs.logits[:, -1, :]\n",
        "        next_token_scores = F.log_softmax(next_token_logits, dim=-1)\n",
        "\n",
        "        # add previous beam_scores to the next_token_scores\n",
        "        # NOTE: size of new_scores: (batch_size * num_beams, vocab_size)\n",
        "        new_scores = next_token_scores + unfinished_beam_scores.unsqueeze(1)\n",
        "\n",
        "        # retrieve top-(2 * num_beams) next tokens for each sample in the batch\n",
        "        # NOTE: size of `top_token_scores` and `top_token_indices` needs to be: (batch_size, 2 * num_beams)\n",
        "        # NOTE: `top_token_scores` and `top_token_indices` should be sorted from the one with larget score to the one with smallest score (for each sample in batch)\n",
        "        # NOTE: new_scores needs to be transformed to shape (batch_size, num_beams * vocab_size) prior to topk operation.\n",
        "        # Hint: use torch.topk with largest=True, sorted=True\n",
        "        top_token_scores, top_token_indices = torch.topk(\n",
        "            new_scores.view(batch_size, num_beams * vocab_size), 2 * num_beams,\n",
        "            dim=1, largest=True, sorted=True\n",
        "        )\n",
        "\n",
        "        # since top_token_indices are over num_beams * vocab_size, divide it by num_beams to get vocabulary index and beam index\n",
        "        top_token_beam_indices = torch.div(top_token_indices, vocab_size,\n",
        "                                           rounding_mode=\"floor\")  # from which beam the top-token was retrieved from\n",
        "        top_token_indices = top_token_indices % vocab_size  # the index of top-token in the vocabulary\n",
        "\n",
        "        # rerank the beam candidates based on neurologic algorithm\n",
        "        rerank_outputs = rerank_beam(new_scores=torch.reshape(new_scores, [batch_size, num_beams, -1]),\n",
        "                                     top_token_beam_indices=top_token_beam_indices,\n",
        "                                     top_token_indices=top_token_indices,\n",
        "                                     constraint_hypotheses=constraints)\n",
        "        top_token_beam_indices, top_token_indices, top_token_scores, constraints = rerank_outputs\n",
        "\n",
        "        # --- Run beam_manager.process and save the results in unfinished_beam_scores, unfinished_token_indices and unfinished_beam_indices --- #\n",
        "        unfinished_beam_outputs = beam_manager.process(\n",
        "            input_ids,\n",
        "            top_token_scores,\n",
        "            top_token_indices,\n",
        "            top_token_beam_indices,\n",
        "            constraints,\n",
        "        )\n",
        "        unfinished_beam_scores = unfinished_beam_outputs[\"unfinished_scores\"]\n",
        "        unfinished_token_indices = unfinished_beam_outputs[\"unfinished_token_indices\"]\n",
        "        unfinished_beam_indices = unfinished_beam_outputs[\"unfinished_beam_indices\"]\n",
        "        constraints = unfinished_beam_outputs[\"unfinished_constraint_hypotheses\"]\n",
        "\n",
        "        # --- Prepare input_ids for next time step --- #\n",
        "        # index input_ids with the unfinished beam indices\n",
        "        # NOTE: input_ids should be (batch_size * num_beams, seq_len)\n",
        "        input_ids = input_ids[unfinished_beam_indices]\n",
        "\n",
        "        # concatenate the unfinished token index to the corresponding input_ids\n",
        "        input_ids = torch.cat([input_ids, unfinished_token_indices.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "        # --- Do not change below --- #\n",
        "        # update the model_kwargs according to the concatenated input_ids\n",
        "        model_kwargs = model._update_model_kwargs_for_generation(\n",
        "            model_outputs, model_kwargs, is_encoder_decoder=False\n",
        "        )\n",
        "        if model_kwargs[\"past_key_values\"] is not None:\n",
        "            model_kwargs[\"past_key_values\"] = model._temporary_reorder_cache(\n",
        "                model_kwargs[\"past_key_values\"], unfinished_beam_indices,\n",
        "            )\n",
        "        # --- Do not change above --- #\n",
        "\n",
        "        # if unfinished input_ids reach the max seq length, exit the loop\n",
        "        if input_ids.size(-1) == max_length:\n",
        "            break\n",
        "\n",
        "    # --- Run beam_manager.finalize to get the best_input_ids and best_scores, among all finished / unfinished beams --- #\n",
        "    best_input_ids, best_scores = beam_manager.finalize(input_ids, unfinished_beam_scores, constraints)\n",
        "\n",
        "    # if len(best_input_ids) < max_length, pad them to the max length\n",
        "    for batch_idx, sample_input_ids in enumerate(best_input_ids):\n",
        "        if sample_input_ids.size(-1) < max_length:\n",
        "            pad_tensor = torch.LongTensor([pad_token_id] * (max_length - sample_input_ids.size(-1))).to(device)\n",
        "\n",
        "            # pad best_input_ids with pad_tensor\n",
        "            best_input_ids[batch_idx] = torch.cat([sample_input_ids, pad_tensor], dim=-1)\n",
        "\n",
        "    # transform best_input_ids (which is currently a list of tensors) into a tensor of size (batch_idx, max_seq_length)\n",
        "    best_input_ids = torch.stack(best_input_ids, dim=0)\n",
        "\n",
        "    return tokenizer.batch_decode(best_input_ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "9JTN6KpYS0P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sanity check for debugging"
      ],
      "metadata": {
        "id": "7Az-Q7QlywzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sents = [\n",
        "    \"The soccer game was tied 3 to 3 and there was a minute left to play.\",\n",
        "    \"Molly loves popcorn.\",\n",
        "    \"Tim rented a car to visit his ill mother.\",\n",
        "]\n",
        "\n",
        "keywords = [\n",
        "    [\"Julie\", \"goal\"],\n",
        "    [\"Molly\", \"mom\"],\n",
        "    [\"Tim\", \"mother\"],\n",
        "]\n",
        "\n",
        "neurologic_search(sents, keywords, num_beams=5, max_length=35)\n",
        "\n",
        "# expected output: ['The soccer game was tied 3 to 3 and there was a minute left to play. The goal was scored by the goalkeeper and the ball was headed for the net. Julie was', 'Molly loves popcorn.\\n\\n\"I love popcorn,\" Molly said. \"I love popcorn mommy.\"\\n', 'Tim rented a car to visit his ill mother.\\n\\n\"She was very upset,\" Tim said. \"She said mother, I\\'m']"
      ],
      "metadata": {
        "id": "fuvQmmYWS3FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Evaluate"
      ],
      "metadata": {
        "id": "BYkg7i5ayyy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Given a list of generations, evaluate their perplexity / fluency / diversity and report the result.\n",
        "\n",
        "def evaluate(generations):\n",
        "  generations = [_ for _ in generations if _ != '']\n",
        "  print(\"Computing perplexity...\")\n",
        "  perplexity = compute_perplexity(generations)\n",
        "  print(\"Computing fleuncy...\")\n",
        "  fluency = compute_fluency(generations)\n",
        "  print(\"Compute diversity...\")\n",
        "  diversity = compute_diversity(generations)\n",
        "  print(\"Neurologic Search\")\n",
        "  print(f'perplexity = {perplexity:.2f}')\n",
        "  print(f'fluency = {fluency:.2f}')\n",
        "  print(f'diversity = {diversity[0]:.2f}, {diversity[1]:.2f}, {diversity[2]:.2f}')\n",
        "  print()"
      ],
      "metadata": {
        "id": "IjPNRdMJud0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If your implementation is efficient enough, the following code will run in no longer than 3 minutes with device = gpu.\n",
        "\n",
        "from tqdm import tqdm\n",
        "from pprint import pprint\n",
        "\n",
        "SUBSET=10\n",
        "NUM_KEYWORDS = 2\n",
        "prompts = [item['prompt'] for item in test_data][:SUBSET]\n",
        "keywords = [item['constraint_words'][:NUM_KEYWORDS] for item in test_data][:SUBSET]\n",
        "\n",
        "MAX_LEN = 50\n",
        "NUM_BEAMS = 5\n",
        "BATCH_SIZE = 5\n",
        "\n",
        "generations = []\n",
        "for batch_start_idx in tqdm(range(0, len(prompts), BATCH_SIZE)):\n",
        "  batched_prompts = prompts[batch_start_idx: batch_start_idx + BATCH_SIZE]\n",
        "  batched_keywords = keywords[batch_start_idx: batch_start_idx + BATCH_SIZE]\n",
        "  batched_generations = neurologic_search(batched_prompts, batched_keywords, NUM_BEAMS, MAX_LEN)\n",
        "\n",
        "  # remove prompt from generation\n",
        "  batched_generations = [generation[len(prompt):] for prompt, generation in zip(batched_prompts, batched_generations)]\n",
        "\n",
        "  generations += batched_generations\n",
        "\n",
        "evaluate(generations)"
      ],
      "metadata": {
        "id": "HGSf-kEUS--e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print first 10 generations\n",
        "\n",
        "sampled_prompts = prompts[:10]\n",
        "sampled_generations = generations[:10]\n",
        "sampled_keywords = keywords[:10]\n",
        "\n",
        "for idx, (prompt, generation, keyword) in enumerate(zip(sampled_prompts, sampled_generations, sampled_keywords)):\n",
        "  print(f\"Prompt {idx}\")\n",
        "  print(prompt)\n",
        "  print(f\"Keyword {idx}\")\n",
        "  print(keyword)\n",
        "  print(f\"Generation {idx}\")\n",
        "  print(generation)\n",
        "  print(\"---------\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "27zicApAtc7Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}