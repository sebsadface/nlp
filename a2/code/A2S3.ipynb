{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "- Before running the jupyter notebook, don't forget to copy it into your drive **(`File` => `Save a copy in Drive`)**. *Failing to do this step may result in losing the progress of your code.*\n",
        "- Change your resource type to GPU before progressing **(`Runtime` => `Change runtime time` => `T4 GPU`).**\n",
        "- There are six steps for this exercise (including step 0, the preperation step). **You will do the following two tasks as detailed under each step, and we will grade both parts:**\n",
        "  - **Coding Exercises:** You will complete the the code blocks denoted by **`TODO:`**.\n",
        "  - **Questions to Answer:** You will answer questions denoted by **`Q:`**.\n",
        "- For the submission of the assignment, please download this notebook as a **Python file**, named `A2S3.py`.\n",
        "\n",
        "# Step 0: Preperation\n",
        "\n",
        "**Step 0.1:** Install dependency and download codebase\n",
        "- This step could take a while.\n",
        "\n",
        "**Step 0.2:** Mount data and files to your drive.\n",
        "- You will see a few popup windows asking for your authorization for this notebook to access your Google Drive files. You need to say yes to all of them."
      ],
      "metadata": {
        "id": "nLDBRhYnS6Pu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKtzKEQ_SCyw"
      },
      "outputs": [],
      "source": [
        "# Step 0.1: Install dependency and download codebase\n",
        "!pip install torch transformers datasets tqdm gdown==v4.6.3\n",
        "!mkdir checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0.2: Mount data and files to your drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "L_4yRXYMIAnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Defining PyTorch Dataset and Dataloader\n",
        "\n",
        "First, you will implement a dataset class (named `SST2Dataset`) for processing the SST-2 dataset. You can find details of the basics of Dataset and Dateloader in this [tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
        "\n",
        "We defined the `SST2Example` class for you, which is used to convert a dict of raw data into an SST2Example object that contains a text and label."
      ],
      "metadata": {
        "id": "Qq_4e2P7FWg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load necessary packages\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import PreTrainedTokenizerFast, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "######################################################\n",
        "#  The following code is given to you.\n",
        "######################################################\n",
        "\n",
        "@dataclass\n",
        "class SST2Example:\n",
        "    \"\"\"\n",
        "    Convert a dict of raw data into an SST2Example object that contains a text and label.\n",
        "    If you're interested, you can find descriptions of dataclass at https://docs.python.org/3/library/dataclasses.html\n",
        "    \"\"\"\n",
        "    text: str\n",
        "    label: int  # 0 for negative, 1 for positive\n",
        "\n",
        "    @staticmethod\n",
        "    def from_dict(data: dict):\n",
        "        text = data['text']\n",
        "        label = data['label']\n",
        "\n",
        "        return SST2Example(\n",
        "            text=text,\n",
        "            label=label,\n",
        "        )\n",
        "\n",
        "\n",
        "def initialize_datasets(tokenizer: PreTrainedTokenizerFast) -> dict:\n",
        "    \"\"\"\n",
        "    Initialize the dataset objects for all splits based on the raw data.\n",
        "    :param tokenizer: A tokenizer used to prepare the inputs for a model (see details in https://huggingface.co/docs/transformers/main_classes/tokenizer).\n",
        "    :return: A dictionary of the dataset splits.\n",
        "    \"\"\"\n",
        "    raw_data = load_dataset(\"gpt3mix/sst2\")\n",
        "    split_datasets = {}\n",
        "\n",
        "    for split_name in raw_data.keys():\n",
        "        split_data = list(raw_data[split_name])\n",
        "\n",
        "        split_datasets[split_name] = SST2Dataset(tokenizer, split_data)\n",
        "\n",
        "    return split_datasets"
      ],
      "metadata": {
        "id": "BZSVUen2Fut9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Coding Exercises** for Step 1:\n",
        "Below, we provide a skeleton for creating a SST-3 Dataset object. **You will complete the following code blocks denoted by `TODO:`.**"
      ],
      "metadata": {
        "id": "5LX8SLHbSyzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SST2Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    Create a customized dataset object for SST-2.\n",
        "    A custom Dataset class must implement three functions: __init__, __len__, and __getitem__.\n",
        "    You can find a detailed tutorial on Dataset at https://pytorch.org/tutorials/beginner/basics/data_tutorial.html.\n",
        "    \"\"\"\n",
        "    tokenizer: PreTrainedTokenizerFast = None\n",
        "\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizerFast, raw_data_list: List[dict]):\n",
        "        SST2Dataset.tokenizer = tokenizer\n",
        "        self.sample_list = [SST2Example.from_dict(data) for data in raw_data_list]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Get the number of items in the dataset.\n",
        "        \"\"\"\n",
        "        # TODO: return the number of samples in sample_list.\n",
        "        return\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get the idx-th item from the dataset.\n",
        "        \"\"\"\n",
        "        # TODO: return the idx-th item in sample_list.\n",
        "        return\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "        Get an iterator for the dataset.\n",
        "        \"\"\"\n",
        "        # TODO: return an iterator for sample_list.\n",
        "        return\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batched_samples: List[SST2Example]) -> dict:\n",
        "        \"\"\"\n",
        "        Encode samples in batched_samples: tokenize the input texts, and turn labels into a tensor.\n",
        "        :param batched_samples: A list of SST2Example samples.\n",
        "        :return: A dictionary of encoded texts and their corresponding labels (in tensors).\n",
        "        \"\"\"\n",
        "        # TODO: collect all input texts from batched_samples into a list.\n",
        "        batched_text =\n",
        "\n",
        "        # TODO: collect all labels from batched_samples into a list.\n",
        "        batched_label =\n",
        "\n",
        "        # Tokenize the input texts.\n",
        "        text_encoding = SST2Dataset.tokenizer(batched_text,\n",
        "                                              padding=True,\n",
        "                                              max_length=512,\n",
        "                                              truncation=True,\n",
        "                                              return_tensors=\"pt\")\n",
        "\n",
        "        # TODO: convert data type of the labels to torch.long (Hint: using torch.LongTensor).\n",
        "        label_encoding =\n",
        "\n",
        "        # TODO: return dictionary of encoded texts and labels.\n",
        "        return"
      ],
      "metadata": {
        "id": "lCfmeRwBK1rD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Questions to Answer** for Step 1:\n",
        "**Answer these questions in your write-up report.**\n",
        "- **Q1.1:** Explain the usages of the following arguments when you encode the input texts: `padding`, `max_length`, `truncation`, `return_tensors`\n",
        "- **Q1.2:** For the above arguments, explain what are the potential advantages of setting them to the default values we provide."
      ],
      "metadata": {
        "id": "nRL-3NzOcFOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Loading Data\n",
        "Here, you will load the data using the Dataloader of the dataset you built from the previous step."
      ],
      "metadata": {
        "id": "ypEhAyqLdfuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Coding Exercises** for Step 2:\n",
        "You will complete the following code blocks denoted by `TODO:`."
      ],
      "metadata": {
        "id": "Snh8dTy2d88M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load train / validation / test dataset, using `initialize_datasets` in `dataset.py`.\n",
        "\"\"\"\n",
        "# TODO: load pre-trained tokenizer for Roberta-base from transformers library.\n",
        "tokenizer =\n",
        "\n",
        "# TODO: load datasets using initialize_datasets.\n",
        "datasets =\n",
        "\n",
        "# TODO: get the first data point in your validation dataset.\n",
        "# Hint: (for you to debug) you returned data point should look like `SST2Example(text=\"It 's a lovely ...\", label=0)`\n",
        "val_first_element =\n",
        "\n",
        "# TODO: get the length of train, validation, and test datasets using `datasets` variable.\n",
        "length_train =\n",
        "length_val =\n",
        "length_test =\n",
        "\n",
        "\"\"\"\n",
        "To load batch of samples from `torch.Dataset` during training / inference, we use `DataLoader` class.\n",
        "Below, we provide an example of loading a dataloader for the validation split of SST-2.\n",
        "\"\"\"\n",
        "validation_dataloader = DataLoader(datasets['validation'],\n",
        "                                   batch_size=64,\n",
        "                                   shuffle=False,\n",
        "                                   collate_fn=SST2Dataset.collate_fn,\n",
        "                                   num_workers=2)\n",
        "\n",
        "# TODO: load the first batch of samples from the validation dataset\n",
        "# Hint: use iterator!\n",
        "batch =\n"
      ],
      "metadata": {
        "id": "qW5cX6L6SuNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Questions to Answer** for Step 2:\n",
        "\n",
        "**Answer these questions in your write-up report.**\n",
        "\n",
        "- **Q2.1:** What are the lengths of train, validation, test datasets?\n",
        "\n",
        "- **Q2.2:** Explain the role of each of the following parameters `batch_size`, `shuffle`, `collate_fn`, `num_workers` given to the `DataLoader` in the above code block. (Hint: You can refer to the Pytorch tutorial on Data processing in the [official website](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).)\n",
        "\n",
        "- **Q2.3:** Write the *type* and *shape* (if the type is tensor) of `input_ids`, `attention_mask`, and `label_encoding` in `batch` and explain *what do these elements represent*."
      ],
      "metadata": {
        "id": "sdK1Wx82Uyut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Training and Evaluation"
      ],
      "metadata": {
        "id": "_EwftWQpXJ-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Optimizer, AdamW\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "kZirwxUXYd6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Coding Exercises** for Step 3:\n",
        "Here, we provide a skeleton for two functions, `train_one_epoch` and `evaluate`. **You will complete code blocks denoted by `TODO:`.**"
      ],
      "metadata": {
        "id": "_Fwo2RQsoeP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model: nn.Module, dataloader: DataLoader, optimizer: Optimizer, epoch: int):\n",
        "    \"\"\"\n",
        "    Train the model for one epoch.\n",
        "    :param model: A pre-trained model loaded from transformers. (e.g., RobertaForSequenceClassification https://huggingface.co/docs/transformers/v4.37.0/en/model_doc/roberta#transformers.RobertaForSequenceClassification)\n",
        "    :param dataloader: A train set dataloader for SST2Dataset.\n",
        "    :param optimizer: An instance of Pytorch optimizer. (e.g., AdamW https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)\n",
        "    :param epoch: An integer denoting current epoch.\n",
        "    Trains model for one epoch.\n",
        "    \"\"\"\n",
        "    # TODO: set the model to the training mode.\n",
        "    model.train()\n",
        "\n",
        "    with tqdm(dataloader, desc=f\"Train Ep {epoch}\", total=len(dataloader)) as tq:\n",
        "        for batch in tq:\n",
        "            # TODO: retrieve the data from your batch and send it to GPU.\n",
        "            # Hint: model.device should point to 'cuda' as you set it as such in the main function below.\n",
        "            text_encoding =\n",
        "            label_encoding =\n",
        "\n",
        "            # TODO: Compute loss by running model with text_encoding and label_encoding.\n",
        "            loss =\n",
        "\n",
        "            # TODO: compute gradients and update parameters using optimizer.\n",
        "            # Hint: you need three lines of code here!\n",
        "\n",
        "\n",
        "            tq.set_postfix({\"loss\": loss.detach().item()}) # for printing better-looking progress bar"
      ],
      "metadata": {
        "id": "o3d2qrp2XPqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model: nn.Module, dataloader: DataLoader) -> float:\n",
        "    \"\"\"\n",
        "    Evaluate model on the dataloader and compute the accuracy.\n",
        "    :param model: A language model loaded from transformers. (e.g., RobertaForSequenceClassification https://huggingface.co/docs/transformers/v4.37.0/en/model_doc/roberta#transformers.RobertaForSequenceClassification)\n",
        "    :param dataloader: A validation / test set dataloader for SST2Dataset\n",
        "    :return: A floating number representing the accuracy of model in the given dataset.\n",
        "    \"\"\"\n",
        "    # TODO: set the model to the evaluation mode.\n",
        "\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with tqdm(dataloader, desc=f\"Eval\", total=len(dataloader)) as tq:\n",
        "        for batch in tq:\n",
        "            with torch.no_grad():\n",
        "                # TODO: retrieve the data from your batch and send it to GPU.\n",
        "                # Hint: model.device should point to 'cuda' as you set it as such in the main function below.\n",
        "                text_encoding =\n",
        "                label_encoding =\n",
        "\n",
        "                # TODO: inference with model and compute logits.\n",
        "                logits =  # Hint: logit should be of size (batch_size, 2)\n",
        "\n",
        "                # TODO: compute list of predictions and list of labels for the current batch\n",
        "                predictions =  # Hint: should be a list [0, 1, ...] of predicted labels\n",
        "                labels =  # Hint: should be a list [0, 1, ...] of ground-truth labels\n",
        "\n",
        "                all_predictions += predictions\n",
        "                all_labels += labels\n",
        "\n",
        "    # compute accuracy\n",
        "    all_predictions = torch.Tensor(all_predictions)\n",
        "    all_labels = torch.Tensor(all_labels)\n",
        "    accuracy = compute_accuracy(all_predictions, all_labels)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def compute_accuracy(predictions: torch.Tensor, labels: torch.Tensor) -> float:\n",
        "    \"\"\"\n",
        "    Given two tensors predictions and labels, compute the accuracy.\n",
        "    :param predictions: torch.Tensor of size (N,)\n",
        "    :param labels: torch.Tensor of size (N,)\n",
        "    :return: A floating number representing the accuracy\n",
        "    \"\"\"\n",
        "    assert predictions.size(-1) == labels.size(-1)\n",
        "\n",
        "    # TODO: compute accuracy\n",
        "    accuracy =\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "i2xd5kiSXJVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Questions to Answer** for Step 3:\n",
        "**Answer these questions in your write-up report.**\n",
        "- **Q3.1:** For the three lines of code you implemented for computing gradients and updating parameters using optimizer, explain what each of the lines does, respectively.\n",
        "\n",
        "- **Q3.2:** Explain what setting the model to training and evaluation modes do, respectively.\n",
        "\n",
        "- **Q3.3:** Explain what `with torch.no_grad()` does in the `evaluation()` function."
      ],
      "metadata": {
        "id": "1zhIKsk-zva0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Main Training Loop\n",
        "\n",
        "The main function uses all the functions implemented above to learn a model for certain number of epochs and evaluate it. Read through the comments and implement `main` that fine-tunes the RoBERTa-based on SST-2."
      ],
      "metadata": {
        "id": "4c64zTgzaKNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Coding Exercises** for Step 4:\n",
        "Here, we provide a skeleton for the `main` function. **You will complete code blocks denoted by `TODO:`.**"
      ],
      "metadata": {
        "id": "biwuzhhYO57P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(64)\n",
        "\n",
        "def main():\n",
        "    # hyper-parameters (we provide initial set of values here, but you can modify them.)\n",
        "    batch_size = 64\n",
        "    learning_rate = 5e-5\n",
        "    num_epochs = 10\n",
        "    model_name = \"roberta-base\"\n",
        "\n",
        "    # TODO: load pre-trained model and corresponding tokenizer (given model_name above).\n",
        "    tokenizer =\n",
        "    model =\n",
        "\n",
        "    # load model on GPU.\n",
        "    model = model.cuda()\n",
        "\n",
        "    # TODO: initialize the AdamW optimizer with optional arguments: lr=learning_rate, eps=1e-8\n",
        "    optimizer =\n",
        "\n",
        "    # TODO: load datasets.\n",
        "    datasets =\n",
        "\n",
        "    # TODO: initialize that training and evaluation (validation / test) dataloaders.\n",
        "    # Hint: you should use the validation dataset during hyperparameter tuning,\n",
        "    # and evaluate the model on the test set once after you finalize the design choice of your model.\n",
        "    # Hint: you should shuffle the training data, but not the validation data.\n",
        "    train_dataloader =\n",
        "    validation_dataloader =\n",
        "\n",
        "    # training loop.\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_one_epoch(model, train_dataloader, optimizer, epoch)\n",
        "        valid_acc = evaluate(model, validation_dataloader)\n",
        "\n",
        "        # TODO: if the newly trained model checkpoint is better than the previously\n",
        "        # saved checkpoint, save the new model in `./checkpoints` folder.\n",
        "        # Hint: remember to update best_acc to the accuracy of the best model so far.\n"
      ],
      "metadata": {
        "id": "DTG5EnW7UyPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the main training loop.\n",
        "# NOTE: if implemented well, each training epoch will take less than 2 minutes.\n",
        "main()"
      ],
      "metadata": {
        "id": "XmMc07CfUtJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Questions to Answer** for Step 4:\n",
        "**Answer these questions in your write-up report.**\n",
        "- **Q4.1:** With the following default hyperparameters we provide, plot both training and validation loss curves across 10 epochs in a single plot (x-axis: num of the epoch; y-axis: acc). You can draw this plot with a Python script or other visualization tools like Google Sheets.\n",
        "\n",
        "  - batch_size = 64\n",
        "  - learning_rate = 5e-5\n",
        "  - num_epochs = 20\n",
        "  - model_name = \"roberta-base\"\n",
        "\n",
        "- **Q4.2:** Describe the behaviors of the training and validation loss curves as you plotted above. At which epoch does the model achieve the best accuracy on the training dataset? What about the validation dataset? Do training and validation curves have the same trend? Why does the current trend happen?\n",
        "\n",
        "- **Q4.3:** Why do you shuffle the training data but not the validation data?\n",
        "\n",
        "- **Q4.4:** Explain the functionality of optimizers.\n",
        "\n",
        "- **Q4.5:** Experiment with two other optimizers defined in `torch.optim` for the training the model with the default hyperparameters we give you. What's the difference between `AdamW` and these two new optimizers? Back up your claims with emperical evidence.\n",
        "\n",
        "- **Q4.6:** Experiment with different combinations of `batch_size`, `learning_rate`, and `num_epochs`. Your goal is to pick the final, best model checkpoint based on the validation dataset accuracy. Describe the strategy you used to try different combinations of hyperparameters. Why did you use this strategy?\n",
        "\n",
        "- **Q4.7:** What are the `batch_size`, `learning_rate`, and `num_epochs` of the best model checkpoint that you picked? What are the training accuracy and validation accuracy?"
      ],
      "metadata": {
        "id": "4fam6lLvAu9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Testing the Final Model"
      ],
      "metadata": {
        "id": "eAJarNtCWoVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Coding Exercises** for Step 5:\n",
        "Here, you load your best trained model from `./checkpoints/` and report the test set accuracy. **You will complete the following code blocks denoted by `TODO:`.**\n",
        "\n"
      ],
      "metadata": {
        "id": "PY34O9hqcyAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# TODO: Load your best trained model from `./checkpoints/` and report the test set accuracy.\n",
        "model =\n",
        "\n",
        "datasets = initialize_datasets(tokenizer)\n",
        "# TODO: Load the test dataset\n",
        "test_dataloader =\n",
        "\n",
        "# TODO: evaluate the model on the test set\n"
      ],
      "metadata": {
        "id": "Xmy87rriepxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Questions to Answer** for Step 5:\n",
        "**Answer these questions in your write-up report.**\n",
        "- **Q5.1:** What's the test set accuracy of the best model?"
      ],
      "metadata": {
        "id": "iXoW_6jEM91z"
      }
    }
  ]
}