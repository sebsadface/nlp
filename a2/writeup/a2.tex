%%%%%%%%%%%%%%%%%%%%% PACKAGE IMPORTS %%%%%%%%%%%%%%%%%%%%%
\documentclass{article}
\usepackage{import}

\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{fullpage}       
\usepackage{changepage}
\usepackage{hyperref}
\usepackage{blindtext}
\usepackage{subcaption}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }
\urlstyle{same}

\newenvironment{level}%
{\addtolength{\itemindent}{2em}}%
{\addtolength{\itemindent}{-2em}}

\usepackage{amsmath,amsthm,amssymb}

\usepackage[nooldvoltagedirection]{circuitikz}
\usetikzlibrary{decorations,arrows,shapes}

\usepackage{datetime}
\usepackage{etoolbox}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{array}
\usepackage{varwidth}
\usepackage{tcolorbox}
\usepackage{amsmath}
\usepackage{circuitikz}
\usepackage{verbatim}
\usepackage[linguistics]{forest}
\usepackage{listings}
\usepackage{xcolor}
\renewcommand{\rmdefault}{cmss}


\newcommand\doubleplus{+\kern-1.3ex+\kern0.8ex}
\newcommand\mdoubleplus{\ensuremath{\mathbin{+\mkern-10mu+}}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    commentstyle=\color{green},
    morecomment=[l][\color{magenta}]{\#},
    backgroundcolor=\color{backcolour},   
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}

\providetoggle{questionnumbers}
\settoggle{questionnumbers}{true}
\newcommand{\noquestionnumbers}{
    \settoggle{questionnumbers}{false}
}

\newcounter{questionCounter}
\newenvironment{question}[2][\arabic{questionCounter}]{%
    \ifnum\value{questionCounter}=0 \else {\newpage}\fi%
    \setcounter{partCounter}{0}%
    \vspace{.25in} \hrule \vspace{0.5em}%
    \noindent{\bf \iftoggle{questionnumbers}{Question #1: }{}#2}%
    \addtocounter{questionCounter}{1}%
    \vspace{0.8em} \hrule \vspace{.10in}%
}

\newcounter{partCounter}[questionCounter]
\renewenvironment{part}[1][\alph{partCounter}]{%
    \addtocounter{partCounter}{1}%
    \vspace{.10in}%
    \begin{indented}%
       {\bf (#1)} %
}{\end{indented}}

\def\indented#1{\list{}{}\item[]}
\let\indented=\endlist
\def\show#1{\ifdefempty{#1}{}{#1\\}}
\def\IMP{\longrightarrow}
\def\AND{\wedge}
\def\OR{\vee}
\def\BI{\leftrightarrow}
\def\DIFF{\setminus}
\def\SUB{\subseteq}


\newcolumntype{C}{>{\centering\arraybackslash}m{1.5cm}}
\renewcommand\qedsymbol{$\blacksquare$}
\newtcolorbox{answer}
{
  colback   = green!5!white,    % Background colorucyitc,
  colframe  = green!75!black,   % Outline color
  box align = center,           % Align box on text line
  varwidth upper,               % Enables multi line input
  hbox                          % Bounds box to text width
}

\newcommand{\myhwname}{CSE 447 Assignment 2}
\newcommand{\myname}{Sebastian Liu}
\newcommand{\myemail}{ll57@cs.washington.edu}
\newcommand{\mysection}{AB}
\newcommand{\dollararrow}{\stackrel{\$}{\leftarrow}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%% Document Options %%%%%%%%%%%%%%%%%%%%%%
\noquestionnumbers
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%% WORK BELOW %%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\begin{center}
    \textbf{Assignment 2} \bigskip
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%% Task 1 %%%%%%%%%%%%%%%%%%%%%%%%M
\begin{question}{Understanding Attentions (20\%)}
    \setcounter{section}{1}
    \setcounter{subsection}{1}
    \subsection{Selection via Attention}
    \begin{part}[1.2.1]
       \begin{answer}
        Assume $q = \begin{bmatrix}
            x_0 & x_1 & x_2
        \end{bmatrix}$ and we have large scalar $S = 1e5$.\\
        We have $A = qK^\top = \begin{bmatrix}
            x_0k_{0,0} + x_1k_{0,1} + x_2k_{0,2} \\
            x_0k_{1,0} + x_1k_{1,1} + x_2k_{1,2} \\
            x_0k_{2,0} + x_1k_{2,1} + x_2k_{2,2} \\
            x_0k_{3,0} + x_1k_{3,1} + x_2k_{3,2} \\
        \end{bmatrix}^\top$,\\
        $A' = \text{softmax}(A) = \begin{bmatrix}
            \frac{\exp( x_0k_{0,0} + x_1k_{0,1} + x_2k_{0,2})}{\sum_{j=0}^{3} \exp(x_0k_{j,0} + x_1k_{j,1} + x_2k_{j,2})} \\
            \frac{\exp(x_0k_{1,0} + x_1k_{1,1} + x_2k_{1,2})}{\sum_{j=0}^{3} \exp(x_0k_{j,0} + x_1k_{j,1} + x_2k_{j,2})} \\
            \frac{\exp(x_0k_{2,0} + x_1k_{2,1} + x_2k_{2,2})}{\sum_{j=0}^{3} \exp(x_0k_{j,0} + x_1k_{j,1} + x_2k_{j,2})} \\
            \frac{\exp(x_0k_{3,0} + x_1k_{3,1} + x_2k_{3,2})}{\sum_{j=0}^{3} \exp(x_0k_{j,0} + x_1k_{j,1} + x_2k_{j,2})} \\
        \end{bmatrix}^\top$, \\
        and finally $O = A' V = \\ \begin{bmatrix}
            \frac{\exp(x_0k_{0,0} + x_1k_{0,1} + x_2k_{0,2})}{\sum_{j=0}^{3} \exp(x_0k_{j,0} + x_1k_{j,1} + x_2k_{j,2})} v_{0,0} + 
            \frac{\exp(x_0k_{1,0} + x_1k_{1,1} + x_2k_{1,2})}{\sum_{j=0}^{3} \exp(x_0k_{j,0} + x_1k_{j,1} + x_2k_{j,2})} v_{1,0} \\+
            \frac{\exp(x_0k_{2,0} + x_1k_{2,1} + x_2k_{2,2})}{\sum_{j=0}^{3} \exp(x_0k_{j,0} + x_1k_{j,1} + x_2k_{j,2})} v_{2,0} +
            \frac{\exp(x_0k_{3,0} + x_1k_{3,1} + x_2k_{3,2})}{\sum_{j=0}^{3} \exp(x_0k_{j,0} + x_1k_{j,1} + x_2k_{j,2})} v_{3,0} \\\\
            \frac{\exp(x_0k_{0,0} + x_1k_{0,1} + x_2k_{0,2})}{\sum_{j=0}^{3} \exp(x_0k_{j,0} + x_1k_{j,1} + x_2k_{j,2})} v_{0,1} +
            \frac{\exp(x_0k_{1,0} + x_1k_{1,1} + x_2k_{1,2})}{\sum_{j=0}^{3} \exp(x_0k_{j,0} + x_1k_{j,1} + x_2k_{j,2})} v_{1,1} \\+
            \frac{\exp(x_0k_{2,0} + x_1k_{2,1} + x_2k_{2,2})}{\sum_{j=0}^{3} \exp(x_0k_{j,0} + x_1k_{j,1} + x_2k_{j,2})} v_{2,1} +
            \frac{\exp(x_0k_{3,0} + x_1k_{3,1} + x_2k_{3,2})}{\sum_{j=0}^{3} \exp(x_0k_{j,0} + x_1k_{j,1} + x_2k_{j,2})} v_{3,1} \\\\
            \frac{\exp(x_0k_{0,0} + x_1k_{0,1} + x_2k_{0,2})}{\sum_{j=0}^{3} \exp(x_0k_{j,0} + x_1k_{j,1} + x_2k_{j,2})} v_{0,2} +
            \frac{\exp(x_0k_{1,0} + x_1k_{1,1} + x_2k_{1,2})}{\sum_{j=0}^{3} \exp(x_0k_{j,0} + x_1k_{j,1} + x_2k_{j,2})} v_{1,2} \\+
            \frac{\exp(x_0k_{2,0} + x_1k_{2,1} + x_2k_{2,2})}{\sum_{j=0}^{3} \exp(x_0k_{j,0} + x_1k_{j,1} + x_2k_{j,2})} v_{2,2} +
            \frac{\exp(x_0k_{3,0} + x_1k_{3,1} + x_2k_{3,2})}{\sum_{j=0}^{3} \exp(x_0k_{j,0} + x_1k_{j,1} + x_2k_{j,2})} v_{3,2} 
 
        \end{bmatrix}^\top$\\\\
        We can see that to get $O = \begin{bmatrix}
            v_{0,0} & v_{0,1} & v_{0,2} \\
        \end{bmatrix}$, we need the first term of $A'$ ($\frac{\exp( x_0k_{0,0} + x_1k_{0,1} + x_2k_{0,2})}{\sum_{j=0}^{3} \exp(x_0k_{j,0} + x_1k_{j,1} + x_2k_{j,2})}$) to be
        significanly larger than the other terms, so the results of $\frac{\exp(x_0k_{0,0} + x_1k_{0,1} + x_2k_{0,2})}{\sum_{j=0}^{3} \exp(x_0k_{j,0} + x_1k_{j,1} + x_2k_{j,2})} v_{0,i}$
        dominate the sum in $O$. \\
        Thus, $\exp(x_0k_{0,0} + x_1k_{0,1} + x_2k_{0,2})$ needs to be significantly larger than the other terms in $A$, to achieve this we need a $q$ that
        scales the first term of $A$ to be larger than the other terms. This means that $q$ needs to be a vector that align with $k_0$ and
        has a large magnitude, then the exponential function will do the amplification. \\
        Therefore, the query vector should be $\boxed{q = S \cdot k_0 = \begin{bmatrix}
            S \cdot k_{0,0} & S \cdot k_{0,1} & S \cdot k_{0,2}
        \end{bmatrix} = \begin{bmatrix}
            0.47S & 0.65S & 0.60S
        \end{bmatrix}}$. By setting a query vector to a specific scaled key vector, we are essentially focusing the model's attention to the corresponding value vectors.

        \end{answer}
    \end{part}
\newpage
    \begin{part}[1.2.2]
        \begin{answer}
            Similar idea to 1.2.1, but instead of the first element of $A'$ dominating the sum in $O$, we want the diagonal elements of $A'$ to dominate the sum of each row
            of $O$. From 1.2.1, we know that we can achieve this by scaling each key vector by a large scalar to focus the model's attention to each of the value vectors. \\
            Therefore, we have:\\ $\boxed{Q = S \cdot K = \begin{bmatrix}
                S \cdot k_0 \\
                S \cdot k_1 \\
                S \cdot k_2 \\
                S \cdot k_3 \\
            \end{bmatrix} = \begin{bmatrix}
                S \cdot k_{0,0} & S \cdot k_{0,1} & S \cdot k_{0,2} \\
                S \cdot k_{1,0} & S \cdot k_{1,1} & S \cdot k_{1,2} \\
                S \cdot k_{2,0} & S \cdot k_{2,1} & S \cdot k_{2,2} \\
                S \cdot k_{3,0} & S \cdot k_{3,1} & S \cdot k_{3,2} \\
            \end{bmatrix} = \begin{bmatrix}
                \phantom{-}0.47S &  \phantom{-}0.65S &  \phantom{-}0.60S \\
                \phantom{-}0.64S &  \phantom{-}0.50S & -0.59S \\
                -0.03S & -0.48S & -0.88S \\
                \phantom{-}0.43S & -0.83S &  \phantom{-}0.35S \\
            \end{bmatrix}}$
        \end{answer}
     \end{part}

     \begin{part}[1.2.3]
        \begin{answer}
            It allows language models to highlight the important parts of a sequence, so the model can directly reference specific details from the input into the output, 
            which might improve accuracy, and appropriateness in certain contexts. It might also give better continuity in longer outputs.
         \end{answer}
     \end{part}
\end{question}

\subsection{Averaging via Attention}
\begin{part}[1.3.1]
    \begin{answer}
        Referring to the calculation in 1.2.1, we can see that if we want to average all the value vectors, we need each element of $A'$ to be
        roughly equal. This means we need $x_0k_{0,0} + x_1k_{0,1} + x_2k_{0,2}$ to be roughly equal to $x_0k_{1,0} + x_1k_{1,1} + x_2k_{1,2}$ and so on. \\
        We see a simple way to achieve this is to set each element in $q$ to be the mean of each corresponding column in $K$ and we can also scale it down to further
        to reduce the variance. So we can have $\mu_{k_{,0}}k_{0,0} + \mu_{k_{,1}}k_{0,1} + \mu_{k_{,2}}k_{0,2}$ be roughly equal
        to $\mu_{k_{,0}}k_{1,0} + \mu_{k_{,1}}k_{1,1} + \mu_{k_{,2}}k_{1,2}$ and so on.\\

        Therefore, we have:\\ $\boxed{q = \frac{k_1 + k_2 + k_3 + k_4}{4} = \begin{bmatrix}
            \frac{0.47 + 0.64 - 0.03 + 0.43}{4} & \frac{0.65 + 0.50 - 0.48 - 0.83}{4} & \frac{0.60 - 0.59 - 0.88 + 0.35}{4}
        \end{bmatrix}}$. \\
    \end{answer}
 \end{part}

 \begin{part}[1.3.2]
    \begin{answer}
        Similar to 1.3.1, but instead of taking the mean of columns of all rows (key vectors) in $K$, we only take the mean of the first two rows (key vectors) in $K$.
        Depending on the result, we could scale up the query vector to slightly  increase the attention on the first two value vectors. \\
        Therefore, we have:\\ $\boxed{q = \frac{k_1 + k_2}{2} = \begin{bmatrix}
            \frac{0.47 + 0.64}{2} & \frac{0.65 + 0.50}{2} & \frac{0.60 - 0.59}{2}
        \end{bmatrix}}$. \\
     \end{answer}
 \end{part}
\newpage
 \begin{part}[1.3.3]
    \begin{answer}
        It gives the model ability to condense different information from a sequence, which allows the model to
        better capture main ideas of a text. It might be good for summarization tasks.
     \end{answer}
 \end{part}

%%%%%%%%%%%%%%%%%%%%%%%% Task 2 %%%%%%%%%%%%%%%%%%%%%%%%
\begin{question}{2 Building Your Own Mini Transformer (40\%)}
    \setcounter{section}{2}
    \setcounter{subsection}{1}
    \subsection{Experiment with Your Implementation of Attention}
    \subsubsection{Experiment 1: Dot-Product Attention v.s. Additive Attention}
    \begin{part}[2.2.1.1 Setup]
        \begin{answer}
            Attention mechanisms allow models to dynamically focus on different parts of the input data when producing an output.
            This experiment compares two attention mechanisms, dot-product attention and additive attention,
            and try to understand how different attention mechanisms affect the performance in the context of a mini-GPT model. \\\\
            Both models has the same architecture except for how they compute the attention scores:\\\\
            \textbf{Dot-product attention} computes the attention scores based on the dot product of query and key vectors and then scaled down with a scale factor. Assume
            we have query matrix $Q$, key matrix $K$, embedding dimensionality $n_{embd}$, and number of heads $n_{head}$, then the attention score matrix $A_{\text{dot-product}}$, is computed as follows:
            $$A_{\text{dot-product}} = \frac{QK^\top}{\sqrt{n_{embd}/n_{head}}}$$

            \textbf{Additive attention} computes attention scores by adding the query and key vectors followed by a nonlinear tanh transformation. Here, the attention score matrix $A$ is computed as follows:
            $$A_{\text{additive}} = W_2 (\tanh(W_1([Q;K])))$$ where $W_1$ and $W_2$ are learned weight matrices. \\\\

            Both models are trained on the same dataset which has the following properties:
            \begin{itemize}
                \item total training tokens: 1,561,375
                \item \# of unique tokens: 80,663
                \item sequence length (in tokens) distribution :\\
                \includegraphics[width=0.44\linewidth]{sequence_length_in_the_train_dataset_by_tokens.png}
            \end{itemize}

         \end{answer}
     \end{part}
     \begin{part}[2.2.1.1 contd.]
        \begin{answer}
            Since a majority of tokens are below 100 tokens, we truncate to 100 tokens for performance reasons.
            Both models has 1.41 million parameters, and are trained for 1 epoch with a batch size of 32. The learning rate is set to 5e-4.\\\\
            My hypothesis is that dot-product attention will compute faster given its simplicity, while additive attention may
            give a slight improvement performance, since its nonlinearity gives it the ability to model more complex relationships.
        \end{answer}
     \end{part}

     \begin{part}[2.2.1.2 Results]
        \begin{answer}
                Table 1: Training Time and Loss Comparison\\
            \begin{tabular}{|c|c|c|}
                \hline
                \textbf{} & \textbf{Dot-Product} & \textbf{Additive Attention} \\
                \hline
                \textbf{Training Time} &  3:35 &  20:29 \\
                \hline
                \textbf{Final Training Loss} &  5.85 &  5.72\\
                \hline
                \textbf{Traing Step vs. Loss} & \includegraphics[width=0.3\linewidth]{../experiments/wo_changes/training_step_vs_loss.png} &  \includegraphics[width=0.3\linewidth]{../experiments/additive_attention/training_step_vs_loss.png}\\
                \hline
            \end{tabular}\\\\
            
            Table 2: Testing Loss Comparison\\
            \#1: "After learning language models model natural language."\\
            \#2: "The quick brown fox jumps over the lazy dog."\\
            \#3: "Amidst the twilight, the ancient ruins whispered secrets long forgotten."\\
            \#4: "Quantum entanglement fascinates physicists and philosophers alike, bridging worlds with invisible threads."\\
            \begin{tabular}{|c|c|c|}
                \hline
                \textbf{} & \textbf{Dot-Product} & \textbf{Additive Attention} \\
                \hline
                \textbf{Sentence\#1} & 9.459  & 9.114  \\
                \hline
                \textbf{Sentence\#2} & 6.742  & 6.478  \\
                \hline
                \textbf{Sentence\#3} & 8.199  & 7.895  \\
                \hline
                \textbf{Sentence\#4} & 6.355  & 6.327 \\
                \hline
            \end{tabular}\\\\

            Table 3: Perplexity Comparison\\
            \begin{tabular}{|c|c|c|}
                \hline
                \textbf{} & \textbf{Dot-Product} & \textbf{Additive Attention} \\
                \hline
                \textbf{Sentence\#1} & 12820.091  &  9083.743 \\
                \hline
                \textbf{Sentence\#2} &  847.358 &  650.523 \\
                \hline
                \textbf{Sentence\#3} &  3636.979 &  2684.190 \\
                \hline
                \textbf{Sentence\#4} &  575.407 &  559.330 \\
                \hline
                \textbf{Train Perplexity} &  401.043 &  405.255 \\
                \hline
                \textbf{Dev Perplexity} &  380.805 &  385.998 \\
                \hline
            \end{tabular}
         \end{answer}
     \end{part}
\newpage
\begin{part}[2.2.1.2 contd.]
    \begin{answer}
        Sample Outputs Comparison:\\
        \textbf{Dot-Product Attention:}
        \begin{enumerate}
            \item $<$START$>$ A age on evidence care nothing , United States are up the people , which is by the attacks for a barrel to big spending on this media when confirm the pledged , Price , low sea after the bring her Life , but she works in Steven Sox media , however . became a presence and then them attempt in December with an discovered of West organization days said , 2009 . $<$STOP$>$
            \item $<$START$>$ And over the House will keep Gordon tour by the department of all its being Big taxable many months were $<$UNK$>$ , on its Radio feed in the new building . $<$STOP$>$
            \item $<$START$>$ dramatic Ireland cents in the New candidates which could take the owners previously sent much been linked to $<$UNK$>$ , rise . $<$STOP$>$
            \item $<$START$>$ A eyes of photos is explained the riding , with their negligence stripped of Afghan 2008 program of 2008 . $<$STOP$>$
            \item $<$START$>$ After Somali government of print broke out the final two documentary Alinghi and $<$UNK$>$ - 4-3 oil will be child and strikes in Wednesday . $<$STOP$>$
        \end{enumerate}
        \textbf{Additive Attention:}
        \begin{enumerate}
            \item $<$START$>$ " I were only that may should not the violence only much time for something president are at particles it 's proposal . $<$STOP$>$
            \item $<$START$>$ All of taking European band perceived $<$UNK$>$ $<$UNK$>$ , Antarctic and the third relatives as fans UCLA place , a profits were singer dangerous . $<$STOP$>$
            \item $<$START$>$ Anbar , received an threat of them on by out up by much parents , reaching out the church agent . $<$STOP$>$
            \item $<$START$>$ tunnels and earnings Bush soldiers are 20 points to reforms -- but today . $<$STOP$>$
            \item $<$START$>$ He is include estimates of your ; stuff managed to fix to answer scandal . $<$STOP$>$
        \end{enumerate}
    \end{answer}
\end{part}

     \begin{part}[2.2.1.3 Explantaion and Interpretation]
        \begin{answer}
            \textbf{Training Time and Loss:}
            Table 1 indicates that models using dot-product attention required significantly less training time (3:35) compared to additive attention (20:29).
            From the training step vs. loss plots, we can see that the loss of both models decreases around the same rate.
            However, additive attention achieved a slightly lower final training loss (5.72) compared to dot-product attention (5.85). This slight difference might
            not speak much about the performance of the models, but when changes the hyperparameters, the final loss stayed consistently lower for the additive attention model (data not shown here),
            which might suggest that the additive attention model is slightly better at capturing the relationships between the tokens in the training data. However, given the significant 
            increase in training time, the improvement in performance might not be worth the trade-off.
         \end{answer}
     \end{part}
\newpage
     \begin{part}[2.2.1.3 contd.]
        \begin{answer}
            \textbf{Testing Loss and Perplexity:}
            Table 2 and Table 3 present the loss and perplexity comparisons. Same story goes here, across four different sentences,
            additive attention consistently outperformed dot-product attention in terms of both lower testing loss and perplexity by a tiny margin.
            However, additive attention did have slightly higher training and dev perplexity (405.255 and 385.998) compared to dot-product attention (401.043 and 380.805),
            which might suggest that the model is overfitting the training data.
            In general, the differences in loss and perplexity are not significant enough to suggest that additive attention is significantly better than dot-product attention. \\\\

            \textbf{Sample Outputs:}
            Both models produced outputs that are mostly nonsensical, but one observation is that the outputs from dot-product attention are slightly more
            coherent than those from additive attention. Also, dot-product attention seems to produce longer sequences on average than additive attention.\\\\

            \textbf{Conclusion:}
            In the context of this experiment, dot-product attention is simpler, more efficient, and produces comparable results to additive attention.
            While additive attention demonstrates slightly superior performance in terms of loss and perplexity, it took significantly longer to train.
            In theory, additive attention should be able to model more complex relationships between tokens due to its nonlinearity, but the results of this experiment
            suggest that the performance improvement is not significant enough to justify the trade-off in training time.
            One possible explanation for the not so significant improvement in performance is that the training data used in this experiment
            might not be complex enough to fully leverage the modeling capabilities of additive attention. Given the restraint in computational resources and time in
            this experiment, dot-product attention is a more practical choice for the mini-GPT model.
            Future work could involve training the models on more complex datasets to see if additive attention can demonstrate more significant performance improvements.
        \end{answer}
     \end{part}
     \subsubsection{Experiment 2: Do We Need All Three of Q, K, and V?}
            \begin{answer}
                \textbf{Setup:}
                Similar to 2.2.1.1, we compare three models with the same architecture (both with dot-product attention) except for model 1
                has all three Q, K, and V used different projections, model 2 has only Q and K sharing the same projection, and model 3 has all three Q, K, and V sharing the same projection.\\\\
                \textbf{Results and Interpretation:}
                We failed to produce any results with any meaningful differences among the three models in terms of training time, loss, perplexity, and sample outputs.
                Even with tuning the hyperparameters (learning rate, batch size, training time, embedding dimensionality etc.),
                all three models produced almost identical results, which might suggest that in the context of this experiment, the model's performance is not sensitive to the
                different projection methods used for Q, K, and V. This might be due to the simplicity of the dataset used in this experiment, which might not require the model to
                fully leverage the different projection methods to capture the relationships between tokens.
             \end{answer}
      \newpage
         \subsubsection{Experiment 3: 0 Masking v.s. -Inf Masking}
        \begin{part}[2.2.3.1 Setup]
            \begin{answer}
                Similar to 2.2.1.1, we compare two models with the same architecture (both with dot-product attention) except for model 1
                used the naive masking approach of setting attention values to 0 after the softmax operation, and model 2 used the more standard approach
                of setting attention values to $-\infty$ before softmax.
             \end{answer}
         \end{part}
    
         \begin{part}[2.2.3.2 Results]
            \begin{answer}
                Table 1: Training Time and Loss Comparison\\
            \begin{tabular}{|c|c|c|}
                \hline
                \textbf{} & \textbf{$-\infty$ Masking} & \textbf{$0$ Masking} \\
                \hline
                \textbf{Training Time} &  3:35 &  2:32 \\
                \hline
                \textbf{Final Training Loss} &  5.85 &  5.27\\
                \hline
                \textbf{Traing Step vs. Loss} & \includegraphics[width=0.3\linewidth]{../experiments/wo_changes/training_step_vs_loss.png} &  \includegraphics[width=0.3\linewidth]{../experiments/masking/training_step_vs_loss.png}\\
                \hline
            \end{tabular}\\\\
            
            Table 2: Testing Loss Comparison \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;  Table 3: Perplexity Comparison\\
            \begin{tabular}{|c|c|c|}
                \hline
                \textbf{} & \textbf{$-\infty$ Masking} & \textbf{$0$ Masking} \\
                \hline
                \textbf{Sentence\#1} & 9.459  & 8.430  \\
                \hline
                \textbf{Sentence\#2} & 6.742  & 5.466  \\
                \hline
                \textbf{Sentence\#3} & 8.199  & 6.271  \\
                \hline
                \textbf{Sentence\#4} & 6.355  & 4.995 \\
                \hline
            \end{tabular}
            \begin{tabular}{|c|c|c|}
                \hline
                \textbf{} & \textbf{$-\infty$ Masking} & \textbf{$0$ Masking} \\
                \hline
                \textbf{Sentence\#1} & 12820.091  &  4584.009 \\
                \hline
                \textbf{Sentence\#2} &  847.358 &  236.421 \\
                \hline
                \textbf{Sentence\#3} &  3636.979 &  528.771 \\
                \hline
                \textbf{Sentence\#4} &  575.407 &  147.611 \\
                \hline
                \textbf{Train Perplexity} &  401.043 &  200.316 \\
                \hline
                \textbf{Dev Perplexity} &  380.805 &  186.542 \\
                \hline
            \end{tabular} \\\\\
            Sample Outputs Comparison:\\
        \textbf{$-\infty$ Masking:} (The same as dot-product attention outputs in 2.2.1.2)\\\\
        \textbf{0 Masking:}
        \begin{enumerate}
            \item $<$START$>$ $<$UNK$>$ . $<$STOP$>$
            \item $<$START$>$ And over " " " " $<$STOP$>$
            \item $<$START$>$ dramatic " " has by " " which could " $<$STOP$>$
            \item $<$START$>$ A " $<$STOP$>$
            \item $<$START$>$ After " " " " " said " I " " " " he I " " will I " " " " " " She 't " not " $<$STOP$>$
        \end{enumerate}
         \end{answer}
         \end{part}
\newpage
         \begin{part}[2.2.3.3 Explanation \& Interpretation]
            \begin{answer}
                \textbf{Training Time and performance:}
                By looking at Table 1, 2 and 3, it seems that the naive $0$ masking approach significantly outperformed the standard $-\infty$ masking approach in terms of both training time and performance.
                0 masking was nearly $30\% $ faster to train and achieved a lower final training loss (5.27) compared to $-\infty$ masking (5.85). 
                The training step vs. loss plots also show that the loss of the 0 masking model converged than the $-\infty$ masking model. The same goes for the testing loss and perplexity,
                where the 0 masking model consistently outperformed the $-\infty$ masking model across four different test sentences and in both training and dev perplexity by a large margin. \\
                Did we just found a new state-of-the-art masking approach? \\\\
                \textbf{Sample Outputs:}
                The answer is NO! The sample outputs from the 0 masking model barely produced any english words, it consist of mostly $<$UNK$>$ tokens and punctuation marks, which suggests that the model
                failed to learn anything meaningful from the training data. This is in stark contrast to the outputs from the $-\infty$ masking model, which although nonsensical, at least contained some meaningful english sequences.\\\\
                \textbf{Interpretation and Conclusion:}
                The results of this experiment suggest that the naive $0$ masking approach does not work for training transformer models. The reason for the "significant improvement" in performance
                is likely due to the zeroing out of the attention scores, which effectively allowed the model to cheat during the test and prevented it from learning anything meaningful from the training data.
            \end{answer}
         \end{part}
\end{question}

%%%%%%%%%%%%%%%%%%%%%%%% Task 3 %%%%%%%%%%%%%%%%%%%%%%%%
\begin{question}{3 HuggingFace (40\%) }
    \setcounter{section}{3}
    \setcounter{subsection}{1}
    \subsection{Finetunning Your Own RoBERTa Model}
    \begin{part}[Q1.1]
        \begin{answer}
            \textbf{padding:} makes sure all the sequences in a batch have the same length by padding the shorter sequences to be the same length as the longest sequence in a batch.\\
            \textbf{max\_length:} defines the max length of the sequences returned by the tokenizer.\\
            \textbf{truncation:} truncates the sequences to the max\_length if they exceed the max\_length.\\
            \textbf{return\_tensors:} specifies the format of the returned tokens. "pt" means that the output should be PyTorch tensors.
        \end{answer}
    \end{part}

    \begin{part}[Q1.2]
        \begin{answer}
            \textbf{padding=True:} making sure all sequences in a batch has the same length allows makes the computation and memory allocation more efficient, since same sized tensors can be processed in parallel.\\
            \textbf{max\_length=512:} 512 seems to be a common empirical choice for balancing the amount of context and computation efficiency. \\
            \textbf{truncation=True:} makes sure we don't get size mismatch, helps with memory and computation efficiency.
            \textbf{return\_tensors="pt":} makes the results compatible with the rest of our PyTorch code, which is convenient since we don't need to manually convert data formats.
        \end{answer}
    \end{part}

    \begin{part}[Q2.1]
        \begin{answer}
            \textbf{length of training set:} 6920 \\
            \textbf{length of validation set:} 872 \\
            \textbf{length of test set:} 1821
        \end{answer}
    \end{part}

    \begin{part}[Q2.2]
        \begin{answer}
            \textbf{batch\_size:} defines the number of samples that will be loaded in each iteration of the training or evaluation.\\
            \textbf{shuffle:} shuffling the order of samples in the dataset in each epoch, which might be good in training to prevent learning from ordering patterns in training data, but here we set to false for validation.\\
            \textbf{collate\_fn:} the function used to encode samples into a batch of tensor, in this case we use the function we implemented in step 1.\\
            \textbf{num\_workers:} the number of processes to use for data loading.
        \end{answer}
    \end{part}
\newpage
    \begin{part}[Q2.3]
        \begin{answer}
            \textbf{input\_ids:} a torch.LongTensor of shape torch.Size([64, 45]) which contains the tokenized representations of the texts (token ids) in the batch.\\
            \textbf{attention\_mask:} a torch.LongTensor of shape torch.Size([64, 45]) which indicates which tokens is valid (masked with 1) and which is not (masked with 0) (paddings).\\
            \textbf{label\_encoding:} a torch.LongTensor of shape torch.Size([64]) which contains the sentiment labels for each sample (0 for negative, 1 for positive).
        \end{answer}
    \end{part}

    \begin{part}[Q3.1]
        \begin{answer}
            \textbf{optimizer.zero\_grad():} clears out the old gradients from the previous batch, which make sure that the gradients doesn't add up across batches. \\
            \textbf{loss.backward():} backpropagation, computes the gradient of the loss w.r.t to trained model parameters.
            \textbf{optimizer.step():} one step of gradient descent, updates the model's parameters based on gradients from loss.backward() to minimize the loss.\\
        \end{answer}
    \end{part}

    \begin{part}[Q3.2]
        \begin{answer}
            \textbf{model.train():} enables training specific behaviors. E.g. dropout some layers to prevent overfitting, and set normalization layers to update their running avgs, etc.\\
            \textbf{model.eval():} disables training specific behaviors.  Making sure that normalization uses its running avgs instead of the current batch's stats which is important for evaluation.
        \end{answer}
    \end{part}

    \begin{part}[Q3.3]
        \begin{answer}
            \textbf{with torch.no\_grad()} improves memory and computation efficiency by not tracking gradients during the forward pass, which is not needed for evaluation.
        \end{answer}
    \end{part}
\newpage
    \begin{part}[Q4.1]
        \begin{answer}
            \includegraphics[width=0.85\linewidth]{./trainvsvalid_acc.png}
        \end{answer}
    \end{part}

    \begin{part}[Q4.2]
        \begin{answer}
            \textbf{Behavior:} Training accuracy increase significantly in the first few epochs, then it starts to plateau around 99\% after the fourth epoch.
            Validation accuracy fluctuates around 92\% throughout the ten epochs, with a drop to 91\% in the last epoch.\\
            \textbf{Best Accuracy: } Training accuracy: 9th epoch, with an accuracy of about 99.3\%.
             Validation accuracy: 6th epoch, with an accuracy of about 93\%.\\

             \textbf{The training and validation accuracy do not follow the same trend.}\\
            \textbf{Reason:}
            This mismatch in trend might be because the model is overfitting to the training data learning the noise with the underlying patterns, which increases training accuracy but not able to generalize.
            That's probably why we see a decline in validation accuracy after the 6th epoch.
        \end{answer}
    \end{part}

    \begin{part}[Q4.3]
        \begin{answer}
            Shuffling the training data helps prevent the model from learning the order of the data, which makes the model generalize better for unseen data.\\
            Not Shuffling in validation data is because we want to consistently evaluate the model's performance on the same sequence of data across different epochs,
            which gives a better comparison of model accuracy.
        \end{answer}
    \end{part}
\newpage
    \begin{part}[Q4.4]
        \begin{answer}
            Optimizers adjust the parameters of the model to minimize the loss function, it updates the model weights based on the gradients from backpropagation.
            Optimizers control the speed and direction of gradient descent (a.k.a learning), which can help model more accurately converges to a optimal.
        \end{answer}
    \end{part}

    \begin{part}[Q4.5]
            \begin{figure}[h]
                \centering
                \begin{subfigure}{.495\textwidth}
                  \centering
                  \includegraphics[width=\linewidth]{./sgd.png}
                  \caption{SGD without Momentum}
                \end{subfigure}
                \begin{subfigure}{.495\textwidth}
                  \centering
                  \includegraphics[width=\linewidth]{./rmsprop.png}
                  \caption{RMSProp}
                \end{subfigure}
            \end{figure}
        \begin{answer}
            Based on the graphs above, we see that with the given hyperparameters AdamW outperforms SGD and RMSprop in terms of training accuracy by a significant margin.
            With the given hyperparameters, SGD see some fluctuation in training accuracy (around 0.518) while validation accuracy stayed exactly the same at 0.509 throughout the epochs. RMSProp
            shows a similar patterns except that the validation accuracy saw a drop in the second epoch to 0.49 and both training and validation accuracy stayed around the 0.51 level throughout the epochs.
            The data shows that with the given hyperparameters, both SGD and RMSProp are not able to further improvements after the first epoch, while AdamW is able to consistently improve the training accuracy.
        \end{answer}
    \end{part}

    \begin{part}[Q4.6]
        \begin{answer}
            I randomly picked four combinations of batch size (32, 64), learning rate (2e-5, 3e-5, 4e-5, 5e-5, 6e-5, 7e-5, 8e-5), and the number
            of epochs (10, 15, 20). (I couldn't go higher for batch size because google colab won't give me enough memory for T4 GPU).\\
            I used random search because I want to try to cover a wider range of combinations with very limited resources.
        \end{answer}
    \end{part}

    \begin{part}[Q4.7]
        \begin{answer}
            The best combination: batch size of 64, learning rate of 3e-5, and 15 epochs. Training Accuracy: 0.996242774566474, Validation Accuracy: 0.9380733944954128.
        \end{answer}
    \end{part}
\newpage
    \begin{part}[Q5.1]
        \begin{answer}
            Best Test Accuracy: 0.9456342668863262
        \end{answer}
    \end{part}
\end{question}
\begin{question}{Acknowledgement}
   
    \begin{answer}
        This assignment was completed by Sebastian Liu.
        ChatGPT 3.5 were used as a human collaborator to ask conceptual questions about dot-product attention, additive attention, attention masking, and the internals
        of different optimizers.
    \end{answer}

\end{question}
\end{document}

